{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c065ae44-9624-4568-8927-3fcb155ca50f",
   "metadata": {},
   "source": [
    "# <span style=\"color: GREEN; font-weight:bold\"> NESTED Returns-Based PWFCV (H-45, OPTIMIZED)\n",
    "\n",
    "## Advanced Time Series Forecasting with Validation\n",
    "\n",
    "### Implemented López de Prado purged walk-forward cross-validation (Rolling Window with overlap - Nested CV):\n",
    "    • Nested CV: Hierarchical validation with 7-10 outer folds, testing generalization across multiple validation sets\n",
    "    • Independent fold scaling: Each CV fold uses separate StandardScaler instances trained only on fold-specific training data\n",
    "    • Unseen validation approach: Validation sets never used in training, ensuring true out-of-sample evaluation\n",
    "    • Temporal barriers: Strict chronological ordering prevents future information leakage into past predictions\n",
    "\n",
    "### RETURN-BASED MODELING: \n",
    "    • Converted prices to percentage returns eliminating non-stationary trends while preserving directional information\n",
    "    • Horizon-adaptive feature selection: Dynamic feature set optimization per forecast period (5d: 45 features, 20d: 48, 45d: 52) maximizing information while minimizing overfitting\n",
    "    • 72 engineered features:\n",
    "     - Technical indicators: SMA (5,10,20,50,200), EMA (12,26), RSI (14), MACD, Bollinger Bands, ATR, momentum, rate of change\n",
    "     - Macro regime signals: VIX (volatility), interest rates, economic calendar events, market breadth indicators\n",
    "     - Statistical features: Rolling volatility, trend strength, autocorrelation, regime detection\n",
    "     \n",
    "### LSTM ARCHITECTURE & OPTIMIZATION:\n",
    "    • Two-layer LSTM with dropout regularization\n",
    "    • Input sequences: 180-day lookback windows with 51 features per timestep\n",
    "    • Output: 45-day multi-step ahead forecasts (single shot prediction)\n",
    "    • Early stopping: Monitor validation loss with 3-epoch patience, restoring best weights preventing overtraining\n",
    "\n",
    "### CUSTOM LOSS FUNCTIONS EVALUATED:\n",
    "    • MSE Loss (Selected): Standard mean squared error on returns, balancing magnitude and direction prediction\n",
    "    • Weighted Directional Loss: Custom loss emphasizing direction correctness with penalties (alpha=10/50, beta=1/5)\n",
    "    • Clipped Weighted Loss: Hybrid approach with gradient clipping preventing extreme predictions (alpha=20, beta=0.5, penalty=2.0)\n",
    "\n",
    "### HYPERPARAMETER OPTIMIZATION:\n",
    "    • Latin Hypercube Sampling: Space-filling design ensuring even coverage of hyperparameter space (10 trials per CV method)\n",
    "    • Search space: LSTM units (32-128), dropout (0.1-0.5), learning rate (1e-5 to 1e-2), batch size (8-64), epochs (10-50)\n",
    "\n",
    "### CONSERVATIVE RISK-AWARE MODELING:\n",
    "    • The model **optimized for direction prediction**, not magnitude prediction.\n",
    "    • Magnitude ratio: 0.45 (predicts 45% of actual price moves)\n",
    "    • Direction Accuracy: 80.77% \n",
    "    • Interpretation: Model intentionally underestimates volatility for safer position sizing in trading applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4bcfb-2976-4321-ace5-e5c60e804996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras import Input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import ta\n",
    "\n",
    "# Disable oneDNN optimizations to avoid potential minor numerical differences caused by floating-point round-off errors.\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71108991-8925-433d-9af2-bf17128ad2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# 1. Data Preparation Function  #####################\n",
    "def prepare_returns_features_OPT(data_df):\n",
    "    \"\"\"\n",
    "    Prepare features for returns-based modeling\n",
    "    Leverages existing _pct() columns and adds minimal transformations\n",
    "    \"\"\"\n",
    "    df = data_df.copy()\n",
    "    \n",
    "    # 1. Rename Close_pct() to target\n",
    "    df = df.rename(columns={'Close_pct()': 'target'})\n",
    "    \n",
    "    # 2. Transform OBV (cumulative → changes)\n",
    "    df['OBV_base_dif'] = df['OBV_base'].diff()\n",
    "    df['OBV_futures_dif'] = df['OBV_futures'].diff()\n",
    "    \n",
    "    # 3. Single list of columns to drop\n",
    "    columns_to_drop = [\n",
    "        'Volume', 'S&P_fut_close', 'S&P_fut_vol',\n",
    "        'GOLD_fut_close', 'DXY_indx_close', 'VIX_indx_close',\n",
    "        'DGS10', 'GDP_value', 'CPIAUCSL_value', \n",
    "        'PAYEMS_value', 'UNRATE_value',\n",
    "        'OBV_base', 'OBV_futures',\n",
    "        'Open', 'High', 'Low'\n",
    "    ]\n",
    "    \n",
    "    # Keep Close for price reconstruction\n",
    "    close_prices = df['Close'].copy()\n",
    "    \n",
    "    # Drop raw levels\n",
    "    features_df = df.drop(columns=['Close'] + columns_to_drop, errors='ignore')\n",
    "     \n",
    "    # 4. Move 'target' to end\n",
    "    cols = [col for col in features_df.columns if col != 'target'] + ['target']\n",
    "    features_df = features_df[cols]\n",
    "\n",
    "    # 5. Drop NaN from diff() operations\n",
    "    features_df.dropna(inplace=True)\n",
    "    close_prices = close_prices.loc[features_df.index]\n",
    "    \n",
    "    print(f\"   Data prepared: {len(features_df.columns) - 1} features, {len(features_df)} samples\")\n",
    "    \n",
    "    # Verify 'target' position \n",
    "    print(f\"   Target at position: {features_df.columns.get_loc('target')} (last)\")\n",
    "    \n",
    "    return features_df, close_prices\n",
    "################################### 1.OPT ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b1209-0fe8-4f40-830a-2c618051718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 2.Scaling Function #######################################\n",
    "def scale_fun_returns_OPT(train_window, validation_window):\n",
    "    \"\"\"\n",
    "    Scaling for returns-based features\n",
    "    Uses StandardScaler for features, StandardScaler for the target\n",
    "    \"\"\"\n",
    "    # Scaler\n",
    "    feature_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "\n",
    "    # Separate features and target\n",
    "    train_features = train_window.drop(columns=['target']).values  # Direct to numpy\n",
    "    train_target = train_window[['target']].values\n",
    "    \n",
    "    val_features = validation_window.drop(columns=['target']).values\n",
    "    val_target = validation_window[['target']].values\n",
    "    \n",
    "    # Scale\n",
    "    train_features_scaled = feature_scaler.fit_transform(train_features)\n",
    "    train_target_scaled = target_scaler.fit_transform(train_target)\n",
    "    \n",
    "    val_features_scaled = feature_scaler.transform(val_features)\n",
    "    val_target_scaled = target_scaler.transform(val_target) \n",
    "      \n",
    "    # Return as numpy arrays (faster for LSTM)\n",
    "    train_scaled = np.hstack([train_features_scaled, train_target_scaled])\n",
    "    val_scaled = np.hstack([val_features_scaled, val_target_scaled])\n",
    "    \n",
    "    return train_scaled, val_scaled, target_scaler, feature_scaler\n",
    "\n",
    "################################### 2.OPT########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db043fd-590f-4544-9ad4-00800adc9bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 3.Sequence Creation ######################################\n",
    "def create_sequences_returns_OPT(train_data, val_data, parameters):\n",
    "    \"\"\"\n",
    "    Create LSTM sequences for returns-based modeling\n",
    "    Target is 'target' (returns) instead of 'Close' (prices)\n",
    "    train_data is a data set of each fold using for training\n",
    "    val_data is a data set of each fold using for validation (y_val only)\n",
    "    \"\"\"\n",
    "    \n",
    "    time_step = parameters['time_step']\n",
    "    horizon = parameters['forecast_horizon']\n",
    "    #target_col = 'target' \n",
    "    \n",
    "    # Calculate number of sequences\n",
    "    # Reserve last time_step days for unseen X_val\n",
    "    # This ensures validation uses truly novel data\n",
    "    # Formula: len - time_step(X) - horizon(y) - time_step(reserve)\n",
    "    n_sequences = len(train_data) - time_step - horizon - time_step\n",
    "    \n",
    "    if n_sequences <= 0:\n",
    "        raise ValueError(f\"Not enough data for sequences. Need {time_step + horizon + time_step} samples.\")\n",
    "    \n",
    "    # PRE-ALLOCATE arrays\n",
    "    n_features = train_data.shape[1]\n",
    "    X_train = np.zeros((n_sequences, time_step, n_features), dtype=np.float32)\n",
    "    y_train = np.zeros((n_sequences, horizon), dtype=np.float32)\n",
    "    \n",
    "    # Target column index (must be the last column)\n",
    "    target_idx = n_features - 1\n",
    "    \n",
    "    # Vectorized sequence creation\n",
    "    for i in range(n_sequences):\n",
    "        X_train[i] = train_data[i : i + time_step]\n",
    "        y_train[i] = train_data[i + time_step : i + time_step + horizon, target_idx]\n",
    "    \n",
    "    # Validation sequences\n",
    "    # Use Last never seen time_step days\n",
    "    X_val = train_data[-time_step:].reshape(1, time_step, n_features)\n",
    "    y_val = val_data[:horizon, target_idx].reshape(1, horizon)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "################################ 3. OPT ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1baf0-a778-45f7-a9bc-481d5d8f6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 4. Split Function (Rolling Window CV with Overlap - Nested) ##################\n",
    "def split_returns_step_size(data_df, parameters, step_size_type='nested', split_prop=0.8, overlap_strategy='adaptive'):\n",
    "    \"\"\"\n",
    "    Split function for returns-based modeling\n",
    "    Nested CV: Training Window slides on Step Size\n",
    "    \"\"\"\n",
    "\n",
    "    train_window_len = parameters['train_window']\n",
    "    time_step = parameters['time_step']\n",
    "    horizon = parameters['forecast_horizon']\n",
    "    embargo_prop = parameters['embargo_prop']\n",
    "\n",
    "    # Fold parameters\n",
    "    validation_len = horizon\n",
    "    purge_len = time_step + horizon - 1\n",
    "    embargo_len = round(purge_len * embargo_prop, ndigits=None) + 1\n",
    "    fold_len = train_window_len + purge_len + validation_len + embargo_len \n",
    "\n",
    "    # Calculate step size (Rolling Window CV with Overlap)\n",
    "    step_size = calculate_step_size_rolling(\n",
    "        horizon=horizon,\n",
    "        embargo_len=embargo_len,\n",
    "        train_window_len=train_window_len,\n",
    "        purge_len=purge_len,\n",
    "        overlap_strategy=overlap_strategy\n",
    "    )\n",
    "#----------------------------------------------------------------------------------------#\n",
    "\n",
    "    data = data_df.copy()\n",
    "    total_len = len(data)\n",
    "    \n",
    "    # Split\n",
    "    train_len = int(total_len * split_prop)\n",
    "    test_len = total_len - train_len\n",
    "    \n",
    "    print(f'Length of the TRAIN DATA SET is: {train_len}')\n",
    "    print(f'Length of the TEST DATA SET is: {test_len}')\n",
    "    \n",
    "    TRAIN_data_set = data.iloc[:train_len]\n",
    "    TEST_data_set = data.iloc[train_len:]\n",
    "    \n",
    "    # Calculate number of folds\n",
    "    available_length = train_len - fold_len\n",
    "    n_folds = max(1, available_length // step_size + 1)\n",
    "\n",
    "    print(f\"\\nRolling Window CV Configuration:\")\n",
    "    print(f\"  Overlap strategy: {overlap_strategy}\")\n",
    "    \n",
    "    print(f\"Fold length = {fold_len}\")\n",
    "    print(f\"Train window = {train_window_len}\")\n",
    "    print(f\"Purge = {purge_len}\")\n",
    "    print(f\"Validation = {validation_len}\")\n",
    "    print(f\"Embargo = {embargo_len}\")\n",
    "    print(f\"Step size = {step_size}\")\n",
    "    print(f'The number of folds: {n_folds}')\n",
    "    print(f\"Time step: {time_step}\")\n",
    "    print(f\"Forecast horizon: {horizon}\")\n",
    "#=========================================================================================== \n",
    "    \n",
    "    # Create folds\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        fold_start = i * step_size\n",
    "        fold_end = fold_start + fold_len\n",
    "\n",
    "        # Check if we have enough data\n",
    "        if fold_start + fold_len > train_len:\n",
    "            print(f\"\\n  Stopping at fold {i}: Not enough data\")\n",
    "            break\n",
    "        \n",
    "        # Training window\n",
    "        train_start = fold_start\n",
    "        train_end = train_start + train_window_len\n",
    "        train_data = data.iloc[train_start:train_end]\n",
    "        \n",
    "        # Purge\n",
    "        purge_start = train_end\n",
    "        purge_end = purge_start + purge_len\n",
    "        \n",
    "        # Validation\n",
    "        val_start = purge_end\n",
    "        val_end = val_start + validation_len\n",
    "        \n",
    "        # Safety Check\n",
    "        if val_end > train_len:\n",
    "            print(f\"\\n Stopping at fold {i}: Validation exceeds available data\")\n",
    "            break\n",
    "\n",
    "        val_data = data.iloc[val_start:val_end]\n",
    "        \n",
    "        # Embargo\n",
    "        embargo_start = val_end\n",
    "        embargo_end = embargo_start + embargo_len\n",
    "        \n",
    "        # Scale each fold using returns-specific scaler\n",
    "        train_scaled, val_scaled, target_scaler, features_scaler = scale_fun_returns_OPT(\n",
    "            train_data, val_data\n",
    "        )\n",
    "        \n",
    "        folds.append({\n",
    "            'fold': i,\n",
    "            'train_data': train_data,\n",
    "            'train_data_scaled': train_scaled,\n",
    "            'validation_data': val_data,\n",
    "            'validation_data_scaled': val_scaled,\n",
    "            'target_scaler': target_scaler,\n",
    "            'features_scaler': features_scaler,\n",
    "            'train_indices': (train_start, train_end),\n",
    "            'validation_indices': (val_start, val_end),\n",
    "            'purge_period': (purge_start, purge_end),\n",
    "            'embargo_period': (embargo_start, embargo_end)\n",
    "        })\n",
    "    \n",
    "    for fold in folds:\n",
    "        print(f\"\\nFold {fold['fold']}:\")\n",
    "        print(f\"  Train indices: {fold['train_indices']}\")\n",
    "        print(f\"  Purge: {fold['purge_period']}\")\n",
    "        print(f\"  Val indices: {fold['validation_indices']}\")\n",
    "        print(f\"  Embargo: {fold['embargo_period']}\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------#\n",
    "    # Verify no validation leakage\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"VALIDATION LEAKAGE CHECK:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    all_safe = True\n",
    "    for i in range(len(folds) - 1):\n",
    "        val_end_i = folds[i]['validation_indices'][1]\n",
    "        embargo_end_i = folds[i]['embargo_period'][1]\n",
    "        val_start_next = folds[i+1]['validation_indices'][0]\n",
    "        \n",
    "        if val_end_i > val_start_next:\n",
    "            print(f\" Fold {i} → {i+1}: LEAKAGE! Val overlap detected\")\n",
    "            all_safe = False\n",
    "        else:\n",
    "            gap = val_start_next - embargo_end_i\n",
    "            print(f\"✓ Fold {i} → {i+1}: Safe (gap after embargo = {gap} days)\")\n",
    "    \n",
    "    if all_safe:\n",
    "        print(f\"\\n✓ All {len(folds)} folds are safe from validation leakage!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "#----------------------------------------------------------------------------------------#\n",
    "    \n",
    "    return folds, TRAIN_data_set, TEST_data_set\n",
    "################################ 4. ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1352e-7dff-4117-97ae-41716cc1dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ 5.PWFCV - LSTM Function ###################################\n",
    "def run_pwfcv_lstm_returns_OPT(data_df, parameters, n_samples=10):\n",
    "    \"\"\"\n",
    "    Run PWFCV for returns-based modeling\n",
    "    \"\"\"\n",
    "    # Split and create folds\n",
    "    folds, _, _ = split_returns_step_size(data_df, parameters)      # Contains  def scale_fun_returns_OPT()\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',          # Monitors validation\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    time_step = parameters['time_step']\n",
    "    horizon = parameters['forecast_horizon']\n",
    "    \n",
    "    best_hp = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    #---------------------------------------------\n",
    "    # Option 1: Generate Random HP combinations\" \n",
    "    #for trial in range(n_samples):\n",
    "    #hp = generate_random_hyperparams_OPT()\n",
    "    #---------------------------------------------\n",
    "    #---------------------------------------------\n",
    "    # Option 2: Latin Hypercube HP Sampling (LHS)   \n",
    "    hp_list = generate_hyperparams_LHS(n_samples=n_samples, seed=42)\n",
    "    for trial, hp in enumerate(hp_list):\n",
    "    #---------------------------------------------\n",
    "        \n",
    "        print(f\"\\nTrial {trial+1}/{n_samples}\")  # Shows progress\n",
    "        print(f\"Evaluating hyperparameters: {hp}\")\n",
    "        \n",
    "        fold_losses = []\n",
    "        # --------------------------\n",
    "        # PWFCV LOOP OVER FOLDS\n",
    "        # --------------------------        \n",
    "        for fold in folds:\n",
    "            train_df = fold['train_data_scaled']\n",
    "            val_df = fold['validation_data_scaled']\n",
    "           \n",
    "            # -------------------------------------------------------------\n",
    "            # Create LSTM sequences ( using 'target' as prediction)\n",
    "            # -------------------------------------------------------------\n",
    "            X_train, y_train, X_val, y_val = create_sequences_returns_OPT(\n",
    "                train_df,\n",
    "                val_df,\n",
    "                parameters\n",
    "            )\n",
    "            \n",
    "            num_features = X_train.shape[-1]\n",
    "\n",
    "            # --------------------------\n",
    "            # Build & train model\n",
    "            # --------------------------           \n",
    "            model = build_lstm_model_OPT(\n",
    "                time_step,\n",
    "                num_features,\n",
    "                horizon,\n",
    "                lstm_units=int(hp['lstm_units']),\n",
    "                dropout_rate=float(hp['dropout']),\n",
    "                learning_rate=float(hp['lr']),\n",
    "                loss_type=hp['loss_type']   # may be 'conservative', 'aggressive', 'clip', 'mse'\n",
    "            )  # Contains  def directional_mse_loss_weighted_OPT() and def directional_mse_loss_clip_weighted()\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=int(hp['epochs']),\n",
    "                batch_size=int(hp['batch_size']),\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks = [early_stop],\n",
    "                verbose=0,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            # --------------------------\n",
    "            # Collect validation loss\n",
    "            # --------------------------\n",
    "            best_val_loss = min(history.history['val_loss'])\n",
    "            fold_losses.append(best_val_loss)\n",
    "\n",
    "            # Memory cleanup\n",
    "            del model\n",
    "            K.clear_session()\n",
    "\n",
    "        mean_loss = np.mean(fold_losses)\n",
    "\n",
    "        print(f\"Mean PWFCV loss: {mean_loss:.6f}\")\n",
    "        \n",
    "        if mean_loss < best_loss:\n",
    "            print(f\"  New best!\")\n",
    "            best_loss = mean_loss\n",
    "            best_hp = hp\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Best hyperparameters: {best_hp}\")\n",
    "    print(f\"Best PWFCV loss: {best_loss:.6f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    return best_hp, folds\n",
    "##################################### 5. ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb87fb0-d36c-4cd0-9ae3-f78c9d137f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### 6.1. Retrain Function ################################\n",
    "def retrain_final_model_returns(train_df, parameters, best_hp,\n",
    "                               use_early_stopping=True):\n",
    "    \"\"\"\n",
    "    Retrain final model on full training set (returns-based)\n",
    "    Args:\n",
    "        train_df: Full training data (80% of dataset)\n",
    "        parameters: Model parameters dict\n",
    "        best_hp: Best hyperparameters from PWFCV\n",
    "        use_early_stopping: If True, split data for early stopping (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained Keras model\n",
    "        target_scaler: StandardScaler for targets\n",
    "        features_scaler: StandardScaler for features\n",
    "    \n",
    "    \"\"\"\n",
    "    time_step = parameters['time_step']\n",
    "    horizon = parameters['forecast_horizon']\n",
    "    \n",
    "    # Scale full training data\n",
    "    # NOTE: scale_fun_returns_OPT requires both train and val data\n",
    "    # We use first 'horizon' rows as dummy validation (not actually used)\n",
    "    full_train_scaled, _, target_scaler, features_scaler = scale_fun_returns_OPT(\n",
    "        train_df,\n",
    "        train_df.iloc[:horizon]  # Dummy val for function signature\n",
    "    )\n",
    "    \n",
    "    # Create sequences\n",
    "    # NOTE: create_sequences_returns_OPT requires both train and val data\n",
    "    # We use first 'horizon' rows as dummy validation (not actually used)\n",
    "    X_train, y_train, _, _ = create_sequences_returns_OPT(\n",
    "        full_train_scaled,\n",
    "        full_train_scaled[:horizon],\n",
    "        parameters\n",
    "    )\n",
    "    \n",
    "    num_features = X_train.shape[-1]\n",
    "    \n",
    "    # Build model\n",
    "    model = build_lstm_model_OPT(\n",
    "        time_step,\n",
    "        num_features,\n",
    "        horizon,\n",
    "        lstm_units=int(best_hp['lstm_units']),\n",
    "        dropout_rate=float(best_hp['dropout']),\n",
    "        learning_rate=float(best_hp['lr']),\n",
    "        loss_type=best_hp['loss_type']   # 'conservative', 'aggressive', 'clip', 'mse'\n",
    "    )  # Contains  def directional_mse_loss_weighted_OPT() and def directional_mse_loss_clip_weighted()\n",
    "    \n",
    "    # Print training info\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING FINAL MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total training samples: {len(X_train)}\")\n",
    "    print(f\"Features: {num_features}\")\n",
    "    print(f\"Time step: {time_step}, Horizon: {horizon}\")\n",
    "    print(f\"\\nBest Hyperparameters:\")\n",
    "    for key, value in best_hp.items():\n",
    "        if key == 'lr':\n",
    "            print(f\"  {key}: {value:.2e}\")\n",
    "        elif key == 'dropout':\n",
    "            print(f\"  {key}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"=\"*50)\n",
    "#--------------------------------------------------------------------------#\n",
    "    # Train with optional early stopping\n",
    "    if use_early_stopping:\n",
    "        # Split 90/10 for early stopping validation\n",
    "        split_idx = int(len(X_train) * 0.9)\n",
    "        X_train_fit = X_train[:split_idx]\n",
    "        y_train_fit = y_train[:split_idx]\n",
    "        X_val_es = X_train[split_idx:]\n",
    "        y_val_es = y_train[split_idx:]\n",
    "        \n",
    "        print(f\"\\nUsing early stopping:\")\n",
    "        print(f\"  Training samples: {len(X_train_fit)}\")\n",
    "        print(f\"  Validation samples: {len(X_val_es)}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train_fit, y_train_fit,\n",
    "            epochs=int(best_hp['epochs']),\n",
    "            batch_size=int(best_hp['batch_size']),\n",
    "            validation_data=(X_val_es, y_val_es),\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1  # Progress bar instead of verbose=2\n",
    "        )\n",
    "        \n",
    "        actual_epochs = len(history.history['loss'])\n",
    "        print(f\"\\nTraining stopped at epoch {actual_epochs}/{best_hp['epochs']}\")\n",
    "        print(f\"Best validation loss: {min(history.history['val_loss']):.6f}\")\n",
    "\n",
    "#--------------------------------------------------------------------------#\n",
    "    # Train on all data without early stopping\n",
    "    else:\n",
    "        print(f\"\\nTraining on ALL data (no early stopping)\")\n",
    "        print(f\"  Training samples: {len(X_train)}\")\n",
    "        print(\"  WARNING: No early stopping - may overfit!\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=int(best_hp['epochs']),\n",
    "            batch_size=int(best_hp['batch_size']),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCompleted all {best_hp['epochs']} epochs\")\n",
    "        print(f\"Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "#--------------------------------------------------------------------------#\n",
    "\n",
    "    print(\"FINAL MODEL TRAINING COMPLETE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return model, target_scaler, features_scaler\n",
    "######################## 6.1. #############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457813a-d6c5-4107-9a85-3061cb242bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Calculate Step_size Function (For Nested CV only) #######################\n",
    "def calculate_step_size_rolling(horizon, embargo_len, train_window_len, \n",
    "                                purge_len, overlap_strategy='adaptive'):\n",
    "    \"\"\"\n",
    "    Calculate step size for ROLLING WINDOW CV WITH OVERLAP (Nested)\n",
    "    This function is ONLY for rolling window approach where:\n",
    "    - Training windows overlap\n",
    "    - Validation windows NEVER overlap (no leakage)\n",
    "    - Step size is between min_safe and max_overlap \n",
    "    Args:\n",
    "        horizon: forecast horizon\n",
    "        embargo_len: embargo period\n",
    "        train_window_len: training window length\n",
    "        purge_len: purge period length\n",
    "        overlap_strategy: 'max_folds', 'adaptive', 'min_overlap'\n",
    "    Returns:\n",
    "        step_size: in range [min_safe_step, max_overlap_step]\n",
    "    \"\"\"\n",
    "    validation_len = horizon\n",
    "    fold_len = train_window_len + purge_len + validation_len + embargo_len\n",
    "    \n",
    "    # ============================================================\n",
    "    # MINIMUM: No validation leakage\n",
    "    # ============================================================\n",
    "    min_safe_step = validation_len + embargo_len\n",
    "    \n",
    "    # ============================================================\n",
    "    # MAXIMUM: Still maintains rolling window with SOME overlap\n",
    "    # ============================================================\n",
    "    # Option 1: Leave at least half the minimum gap\n",
    "    max_overlap_step_v1 = fold_len - (min_safe_step // 2)\n",
    "    \n",
    "    # Option 2: Leave at least the embargo length as overlap\n",
    "    max_overlap_step_v2 = fold_len - embargo_len\n",
    "    \n",
    "    # Option 3: Leave at least validation length as overlap\n",
    "    max_overlap_step_v3 = fold_len - validation_len\n",
    "    \n",
    "    # RECOMMENDED: Option 2 (keeps at least embargo as buffer)\n",
    "    max_overlap_step = max_overlap_step_v2\n",
    "    \n",
    "    # Ensure max > min (safety check)\n",
    "    if max_overlap_step <= min_safe_step:\n",
    "        max_overlap_step = int(min_safe_step * 1.5)\n",
    "    \n",
    "    # ============================================================\n",
    "    # Calculate step size based on strategy\n",
    "    # ============================================================\n",
    "    if overlap_strategy == 'max_folds':\n",
    "        # Maximum folds: Use minimum safe step\n",
    "        step_size = min_safe_step\n",
    "        overlap_description = \"Maximum (most folds)\"\n",
    "        \n",
    "    elif overlap_strategy == 'min_overlap':\n",
    "        # Minimum overlap: Use maximum step while keeping rolling approach\n",
    "        step_size = max_overlap_step\n",
    "        overlap_description = \"Minimum (fewest folds, still overlapping)\"\n",
    "        \n",
    "    elif overlap_strategy == 'adaptive':\n",
    "        # Adaptive based on horizon\n",
    "        if horizon <= 7:\n",
    "            # Short horizon: Need more folds for stability\n",
    "            # Use 40% of range from min\n",
    "            step_size = int(min_safe_step + (max_overlap_step - min_safe_step) * 0.4)\n",
    "        elif horizon <= 20:\n",
    "            # Medium horizon: Moderate overlap\n",
    "            # Use 60% of range from min\n",
    "            step_size = int(min_safe_step + (max_overlap_step - min_safe_step) * 0.6)\n",
    "        elif horizon <= 30:\n",
    "            # Medium-long horizon\n",
    "            # Use 75% of range from min\n",
    "            step_size = int(min_safe_step + (max_overlap_step - min_safe_step) * 0.75)\n",
    "        else:\n",
    "            # Long horizon: Closer to minimum overlap\n",
    "            # Use 20% of range from min\n",
    "            step_size = int(min_safe_step + (max_overlap_step - min_safe_step) * 0.2)\n",
    "        overlap_description = f\"Adaptive for {horizon}-day horizon\"\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown overlap_strategy: {overlap_strategy}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # Safety checks\n",
    "    # ============================================================\n",
    "    step_size = max(min_safe_step, min(step_size, max_overlap_step))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_overlap = max(0, train_window_len - step_size)\n",
    "    train_overlap_pct = (train_overlap / train_window_len * 100) if train_window_len > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ROLLING WINDOW CV - STEP SIZE CALCULATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Strategy: {overlap_strategy} ({overlap_description})\")\n",
    "    print(f\"\\nHorizon: {horizon} days\")\n",
    "    print(f\"Validation length: {validation_len} days\")\n",
    "    print(f\"Embargo length: {embargo_len} days\")\n",
    "    print(f\"Purge length: {purge_len} days\")\n",
    "    print(f\"Train window: {train_window_len} days\")\n",
    "    print(f\"Total fold length: {fold_len} days\")\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"STEP SIZE RANGE (for Rolling Window with Overlap):\")\n",
    "    print(f\"  Min safe step (max folds):  {min_safe_step:4d} days\")\n",
    "    print(f\"  Max overlap step (min folds): {max_overlap_step:4d} days\")\n",
    "    print(f\"  Selected step size:          {step_size:4d} days\")\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"RESULTING OVERLAP:\")\n",
    "    print(f\"  Training window overlap: {train_overlap} days ({train_overlap_pct:.1f}%)\")\n",
    "    print(f\"  Validation overlap: 0 days (0%) ✓ No leakage\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return step_size\n",
    "################################# Calculate Step_size Function #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed463a-3320-49af-aae8-28dfcc2932cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### 6.2. Retrain Function ################################\n",
    "                  ###################### Helper functions ######################\n",
    "def scale_train_only_returns(train_df):\n",
    "    \"\"\"\n",
    "    Scale training data only (for final model training)\n",
    "    This is a dedicated function that doesn't require dummy validation data.\n",
    "    Args:\n",
    "        train_df: Training DataFrame with features + target\n",
    "    Returns:\n",
    "        train_scaled: Scaled training data (numpy array)\n",
    "        target_scaler: Fitted StandardScaler for target\n",
    "        feature_scaler: Fitted StandardScaler for features\n",
    "    \"\"\"\n",
    "   \n",
    "    feature_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Separate features and target\n",
    "    train_features = train_df.drop(columns=['target']).values\n",
    "    train_target = train_df[['target']].values\n",
    "    \n",
    "    # Fit and transform\n",
    "    train_features_scaled = feature_scaler.fit_transform(train_features)\n",
    "    train_target_scaled = target_scaler.fit_transform(train_target)\n",
    "    \n",
    "    # Combine\n",
    "    train_scaled = np.hstack([train_features_scaled, train_target_scaled])\n",
    "    \n",
    "    return train_scaled, target_scaler, feature_scaler\n",
    "\n",
    "\n",
    "def create_sequences_train_only_returns(train_data, parameters):\n",
    "    \"\"\"\n",
    "    Create LSTM sequences for final training only\n",
    "    This is a dedicated function that doesn't require dummy validation data.\n",
    "    Reserves last time_step days for potential future use (consistent with CV).\n",
    "    Args:\n",
    "        train_data: Scaled training data (numpy array)\n",
    "        parameters: Dict with time_step and forecast_horizon\n",
    "    \n",
    "    Returns:\n",
    "        X_train: Training input sequences\n",
    "        y_train: Training target sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    time_step = parameters['time_step']\n",
    "    horizon = parameters['forecast_horizon']\n",
    "    \n",
    "    # Reserve last time_step days (consistent with CV approach)\n",
    "    n_sequences = len(train_data) - time_step - horizon - time_step\n",
    "    \n",
    "    if n_sequences <= 0:\n",
    "        raise ValueError(\n",
    "            f\"Not enough data. Need at least {time_step + horizon + time_step} samples, \"\n",
    "            f\"but got {len(train_data)}\"\n",
    "        )\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    n_features = train_data.shape[1]\n",
    "    X_train = np.zeros((n_sequences, time_step, n_features), dtype=np.float32)\n",
    "    y_train = np.zeros((n_sequences, horizon), dtype=np.float32)\n",
    "    \n",
    "    target_idx = n_features - 1\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(n_sequences):\n",
    "        X_train[i] = train_data[i : i + time_step]\n",
    "        y_train[i] = train_data[i + time_step : i + time_step + horizon, target_idx]\n",
    "    \n",
    "    return X_train, y_train\n",
    "    \n",
    "            ###################################################################\n",
    "               ###################### Main function ######################\n",
    "\n",
    "def retrain_final_model_returns_CLEAN(train_df, parameters, best_hp,\n",
    "                                       use_early_stopping=True):\n",
    "    \"\"\"\n",
    "    Clean version using dedicated helper functions\n",
    "    This version doesn't use dummy validation data - cleaner and more explicit\n",
    "    Args:\n",
    "        train_df: Full training data (80% of dataset)\n",
    "        parameters: Model parameters dict\n",
    "        best_hp: Best hyperparameters from PWFCV\n",
    "        use_early_stopping: If True, split data for early stopping (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained Keras model\n",
    "        target_scaler: StandardScaler for targets\n",
    "        features_scaler: StandardScaler for features\n",
    "    \"\"\"\n",
    "    \n",
    "    time_step = parameters['time_step']\n",
    "    horizon = parameters['forecast_horizon']\n",
    "    \n",
    "    # Scale training data (dedicated function - no dummy validation)\n",
    "    train_scaled, target_scaler, features_scaler = scale_train_only_returns(train_df)\n",
    "    \n",
    "    # Create sequences (dedicated function - no dummy validation)\n",
    "    X_train, y_train = create_sequences_train_only_returns(train_scaled, parameters)\n",
    "    \n",
    "    num_features = X_train.shape[-1]\n",
    "    \n",
    "    # Build model with best hyperparameters\n",
    "    model = build_lstm_model_OPT(\n",
    "        time_step,\n",
    "        num_features,\n",
    "        horizon,\n",
    "        lstm_units=int(best_hp['lstm_units']),\n",
    "        dropout_rate=float(best_hp['dropout']),\n",
    "        learning_rate=float(best_hp['lr']),\n",
    "        loss_type=best_hp['loss_type']   # 'conservative', 'aggressive', 'clip', 'mse'\n",
    "    )\n",
    "    \n",
    "    # Print training info\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING FINAL MODEL (CLEAN VERSION)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total training samples: {len(X_train)}\")\n",
    "    print(f\"Features: {num_features}\")\n",
    "    print(f\"Time step: {time_step}, Horizon: {horizon}\")\n",
    "    print(f\"\\nBest Hyperparameters:\")\n",
    "    for key, value in best_hp.items():\n",
    "        if key == 'lr':\n",
    "            print(f\"  {key}: {value:.2e}\")\n",
    "        elif key == 'dropout':\n",
    "            print(f\"  {key}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Train with optional early stopping\n",
    "    if use_early_stopping:\n",
    "        # Split 90/10 for early stopping validation\n",
    "        split_idx = int(len(X_train) * 0.9)\n",
    "        X_train_fit = X_train[:split_idx]\n",
    "        y_train_fit = y_train[:split_idx]\n",
    "        X_val_es = X_train[split_idx:]\n",
    "        y_val_es = y_train[split_idx:]\n",
    "        \n",
    "        print(f\"\\nUsing early stopping:\")\n",
    "        print(f\"  Training samples: {len(X_train_fit)}\")\n",
    "        print(f\"  Validation samples: {len(X_val_es)}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train_fit, y_train_fit,\n",
    "            epochs=int(best_hp['epochs']),\n",
    "            batch_size=int(best_hp['batch_size']),\n",
    "            validation_data=(X_val_es, y_val_es),\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        actual_epochs = len(history.history['loss'])\n",
    "        print(f\"\\nTraining stopped at epoch {actual_epochs}/{best_hp['epochs']}\")\n",
    "        print(f\"Best validation loss: {min(history.history['val_loss']):.6f}\")\n",
    "    \n",
    "    else:\n",
    "        # Train on all data\n",
    "        print(f\"\\nTraining on ALL data (no early stopping)\")\n",
    "        print(f\"  Training samples: {len(X_train)}\")\n",
    "        print(\"  WARNING: No early stopping - may overfit!\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=int(best_hp['epochs']),\n",
    "            batch_size=int(best_hp['batch_size']),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCompleted all {best_hp['epochs']} epochs\")\n",
    "        print(f\"Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"FINAL MODEL TRAINING COMPLETE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return model, target_scaler, features_scaler\n",
    "\n",
    "######################## 6.2. #############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6963d-7caa-43ab-9129-d8de5282dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## 7.Evaluation on Test Returns #####################################\n",
    "#-------------------------------------\n",
    "# Add Helper Function \n",
    "#-------------------------------------\n",
    "def reconstruct_prices(returns, start_price):\n",
    "    \"\"\"\n",
    "    Reconstruct prices from returns\n",
    "    \n",
    "    Args:\n",
    "        returns: Array of returns (as decimals, e.g., 0.02 = 2%)\n",
    "        start_price: Starting price\n",
    "    \n",
    "    Returns:\n",
    "        Array of prices\n",
    "    \"\"\"\n",
    "    prices = []\n",
    "    current_price = start_price\n",
    "    \n",
    "    for ret in returns:\n",
    "        next_price = current_price * (1 + ret)\n",
    "        prices.append(next_price)\n",
    "        current_price = next_price\n",
    "    \n",
    "    return np.array(prices)\n",
    "\n",
    "#-----------------------------------\n",
    "# Add Visualization \n",
    "#-----------------------------------\n",
    "def plot_first_predictionon_test_set(y_true_prices, y_pred_prices, save_path=None):\n",
    "    '''Plot true vs predicted prices'''\n",
    "    \n",
    "    days = np.arange(len(y_true_prices))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(days, y_true_prices, 'b-', label='True', linewidth=2)\n",
    "    plt.plot(days, y_pred_prices, 'r--', label='Predicted', linewidth=2)\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.title(f'{len(y_true_prices)}-Day Price Forecast')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "#----------------------------------------------------------------------------------#\n",
    "\n",
    "def evaluate_on_test_returns(model, train_df, test_df, train_close_prices, test_close_prices, parameters):\n",
    "    \"\"\"\n",
    "    Evaluate model and reconstruct prices from predicted returns\n",
    "    INPUTS:\n",
    "    -------\n",
    "      model: Trained Keras model\n",
    "      train_df: Training features (4783, 54)\n",
    "      test_df: Test features (1196, 54)\n",
    "      train_close_prices: Training prices (for reconstruction)\n",
    "      test_close_prices: Test prices (for verification)\n",
    "      parameters: Dict with time_step, horizon\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    --------\n",
    "      y_true_returns: True returns (horizon, )\n",
    "      y_pred_returns: Predicted returns (horizon, )\n",
    "      y_true_prices: True prices (horizon, )\n",
    "      y_pred_prices: Predicted prices (horizon, )\n",
    "      target_scaler: For future use\n",
    "    \"\"\"\n",
    "    time_step = parameters['time_step']\n",
    "    horizon = parameters['forecast_horizon']\n",
    "    \n",
    "    # Scale data\n",
    "    train_scaled, test_scaled, target_scaler, features_scaler = scale_fun_returns_OPT(\n",
    "        train_df, test_df\n",
    "    )\n",
    "    \n",
    "    # Build test sequence    \n",
    "    X_test = train_scaled[-time_step:].reshape(1, time_step, -1)\n",
    "    \n",
    "    # Get true returns (scaled)\n",
    "    y_test_true_scaled = test_scaled[:horizon, -1]  # Last column = target\n",
    "    \n",
    "    # Predict returns (scaled)\n",
    "    y_test_pred_scaled = model.predict(X_test, verbose=0)[0]\n",
    "    \n",
    "    # Inverse transform to get actual returns\n",
    "    y_true_returns = target_scaler.inverse_transform(\n",
    "        y_test_true_scaled.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    y_pred_returns = target_scaler.inverse_transform(\n",
    "        y_test_pred_scaled.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    # Reconstruct prices from returns\n",
    "    last_train_price = train_close_prices.iloc[-1]\n",
    "    \n",
    "    # True prices\n",
    "    y_true_prices = reconstruct_prices(y_true_returns, last_train_price)\n",
    "    # Predicted prices\n",
    "    y_pred_prices = reconstruct_prices(y_pred_returns, last_train_price)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RETURNS (Scaled):\")\n",
    "    print(f\"TRUE: {y_test_true_scaled}\")\n",
    "    print(f\"PRED: {y_test_pred_scaled}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RETURNS (Original %):\")\n",
    "    print(f\"TRUE: {y_true_returns * 100}%\")\n",
    "    print(f\"PRED: {y_pred_returns * 100}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PRICES (Reconstructed $):\")\n",
    "    print(f\"TRUE: {y_true_prices}\")\n",
    "    print(f\"PRED: {y_pred_prices}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"FIRST PREDICTION (45-day forecast)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\\\nRETURNS (Original %):\")\n",
    "    print(f\"TRUE: Mean={np.mean(y_true_returns)*100:.2f}%, Std={np.std(y_true_returns)*100:.2f}%\")\n",
    "    print(f\"PRED: Mean={np.mean(y_pred_returns)*100:.2f}%, Std={np.std(y_pred_returns)*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"-\"*50)\n",
    "    print(\"PRICES (Reconstructed $):\")\n",
    "    print(f\"Start: ${last_train_price:.2f}\")\n",
    "    print(f\"True End: ${y_true_prices[-1]:.2f}\")\n",
    "    print(f\"Pred End: ${y_pred_prices[-1]:.2f}\")\n",
    "    print(\"\\\\n\" + \"-\"*50)\n",
    "    \n",
    "    # Calculate metrics on prices\n",
    "    mae = np.mean(np.abs(y_true_prices - y_pred_prices))\n",
    "    mape = np.mean(np.abs((y_true_prices - y_pred_prices) / y_true_prices)) * 100\n",
    "    rmse = np.sqrt(np.mean((y_true_prices - y_pred_prices) ** 2))\n",
    "    \n",
    "    print(f\"\\nMetrics (Price Scale):\")\n",
    "    print(f\"MAE:  ${mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"RMSE: ${rmse:.2f}\")\n",
    "    \n",
    "    # Direction accuracy\n",
    "#------------------------------------------------------------------------------\n",
    "    print(f\"\\\\n{'='*50}\")\n",
    "   \n",
    "    true_direction = y_true_prices[-1] > y_true_prices[0]  # Overall H-days\n",
    "    pred_direction = y_pred_prices[-1] > y_pred_prices[0]\n",
    "    \n",
    "    # Also add cumulative return:\n",
    "    true_cumulative = (y_true_prices[-1] / y_true_prices[0]) - 1\n",
    "    pred_cumulative = (y_pred_prices[-1] / y_pred_prices[0]) - 1\n",
    "    \n",
    "    print(f\"\\nCumulative Return (45 days):\")\n",
    "    print(f\"  True: {true_cumulative*100:+.2f}%\")\n",
    "    print(f\"  Pred: {pred_cumulative*100:+.2f}%\")\n",
    "    print(f\"  Direction: {'✓ CORRECT' if np.sign(true_cumulative) == np.sign(pred_cumulative) else '✗ INCORRECT'}\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "    # Visualization\n",
    "    plot_first_predictionon_test_set(y_true_prices, y_pred_prices, save_path=None)\n",
    "    \n",
    "    return y_true_returns, y_pred_returns, y_true_prices, y_pred_prices, target_scaler\n",
    "##################### 7. new ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191d4bd-d506-4a1d-a1af-51f357c8e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### 8. Rolling Evaluation for Full Test Set ##############################\n",
    "def evaluate_rolling_test_returns_OPT(model, train_df, test_df, train_close_prices, test_close_prices, parameters):\n",
    "    \"\"\"\n",
    "    Rolling window evaluation on test set with price reconstruction\n",
    "    NPUTS:\n",
    "    -------\n",
    "      model: Trained final model\n",
    "      train_df: Training features (4783, 54)\n",
    "      test_df: Test features (1196, 54)\n",
    "      train_close_prices: Training prices\n",
    "      test_close_prices: Test prices\n",
    "      parameters: Dict with time_step, horizon\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    --------\n",
    "      actuals_returns: (n_predictions, horizon) - true returns\n",
    "      predictions_returns: (n_predictions, horizon) - predicted returns\n",
    "      actuals_prices: (n_predictions, horizon) - true prices\n",
    "      predictions_prices: (n_predictions, horizon) - predicted prices\n",
    "      target_scaler: For reference\n",
    "    \"\"\"\n",
    "    time_step = parameters['time_step']\n",
    "    horizon = parameters['forecast_horizon']\n",
    "    \n",
    "    # Scale data\n",
    "    train_scaled, test_scaled, target_scaler, _ = scale_fun_returns_OPT(train_df, test_df)\n",
    "    \n",
    "    # Combine for rolling window\n",
    "    combined = np.vstack([train_scaled, test_scaled])\n",
    "    combined_prices = pd.concat([train_close_prices, test_close_prices], axis=0).values\n",
    "    \n",
    "    train_len = len(train_scaled)\n",
    "    n_predictions = (len(test_scaled) - horizon) // horizon + 1\n",
    "\n",
    "    # PRE-ALLOCATE arrays for all predictions\n",
    "    all_predictions = np.zeros((n_predictions, horizon))\n",
    "    all_actuals = np.zeros((n_predictions, horizon))\n",
    "    last_prices = np.zeros(n_predictions)\n",
    "    \n",
    "    print(f\"\\nRunning rolling evaluation on test set...\")\n",
    "    print(f\"Making {n_predictions} predictions ({horizon}-day each)\")\n",
    "\n",
    "    # Collect all predictions\n",
    "    pred_idx = 0\n",
    "    for i in range(0, len(test_scaled) - horizon + 1, horizon):\n",
    "        window_start = train_len + i - time_step\n",
    "        window_end = train_len + i\n",
    "        \n",
    "        if window_start < 0:\n",
    "            continue\n",
    "        \n",
    "        # Extract and predict\n",
    "        X_test = combined[window_start:window_end].reshape(1, time_step, -1)\n",
    "        y_pred_scaled = model.predict(X_test, verbose=0)[0]\n",
    "        y_true_scaled = test_scaled[i:i+horizon, -1]  # Last column is target\n",
    "        \n",
    "        all_predictions[pred_idx] = y_pred_scaled\n",
    "        all_actuals[pred_idx] = y_true_scaled\n",
    "        start_price_idx = window_end - 1\n",
    "        last_prices[pred_idx] = combined_prices[start_price_idx]\n",
    "        pred_idx += 1\n",
    "        \n",
    "        #-----------------------------------------------------\n",
    "        # Progress indicator\n",
    "        if pred_idx % 5 == 0 or pred_idx == n_predictions:\n",
    "            print(f\"  Completed {pred_idx}/{n_predictions} predictions...\")\n",
    "        #-----------------------------------------------------\n",
    "    \n",
    "    # Trim to actual predictions made\n",
    "    all_predictions = all_predictions[:pred_idx]\n",
    "    all_actuals = all_actuals[:pred_idx]\n",
    "    last_prices = last_prices[:pred_idx]\n",
    "    \n",
    "    # Batch inverse transform\n",
    "    actuals_returns = target_scaler.inverse_transform(\n",
    "        all_actuals.reshape(-1, 1)\n",
    "    ).reshape(-1, horizon)\n",
    "    \n",
    "    predictions_returns = target_scaler.inverse_transform(\n",
    "        all_predictions.reshape(-1, 1)\n",
    "    ).reshape(-1, horizon)\n",
    "    \n",
    "    # VECTORIZED price reconstruction (no loops!)\n",
    "    # For each prediction window, compound returns into prices\n",
    "    actuals_prices = np.zeros_like(actuals_returns)\n",
    "    predictions_prices = np.zeros_like(predictions_returns)\n",
    "    \n",
    "    for i in range(pred_idx):\n",
    "        # Cumulative product of (1 + return)\n",
    "        actual_multipliers = np.cumprod(1 + actuals_returns[i])\n",
    "        pred_multipliers = np.cumprod(1 + predictions_returns[i])\n",
    "        \n",
    "        actuals_prices[i] = last_prices[i] * actual_multipliers\n",
    "        predictions_prices[i] = last_prices[i] * pred_multipliers\n",
    "    \n",
    "    print(f\"Completed {pred_idx} predictions\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    actuals_flat = actuals_prices.flatten()\n",
    "    predictions_flat = predictions_prices.flatten()\n",
    "    \n",
    "    mae = np.mean(np.abs(actuals_flat - predictions_flat))\n",
    "    mape = np.mean(np.abs((actuals_flat - predictions_flat) / actuals_flat)) * 100\n",
    "    rmse = np.sqrt(np.mean((actuals_flat - predictions_flat) ** 2))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Direction accuracy\n",
    "    direction_correct = 0\n",
    "    for i in range(pred_idx):\n",
    "        true_direction = actuals_prices[i][-1] > actuals_prices[i][0]\n",
    "        pred_direction = predictions_prices[i][-1] > predictions_prices[i][0]\n",
    "        if true_direction == pred_direction:\n",
    "            direction_correct += 1\n",
    "    \n",
    "    direction_accuracy = direction_correct / pred_idx\n",
    "    \n",
    "    # ADDED: Per-window statistics\n",
    "    per_window_mae = np.mean(np.abs(actuals_prices - predictions_prices), axis=1)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\\\n{'='*70}\")\n",
    "    print(f\"ROLLING EVALUATION RESULTS ({pred_idx} predictions)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\\\nTest Set Metrics:\")\n",
    "    print(f\"  Direction Accuracy: {direction_accuracy*100:.2f}% \" + \n",
    "          f\"({direction_correct}/{pred_idx} correct)\")\n",
    "    print(f\"  MAE:  ${mae:.2f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    print(f\"  RMSE: ${rmse:.2f}\")\n",
    "    \n",
    "    print(f\"\\\\nPer-Window MAE Statistics:\")\n",
    "    print(f\"  Best:   ${np.min(per_window_mae):.2f}\")\n",
    "    print(f\"  Worst:  ${np.max(per_window_mae):.2f}\")\n",
    "    print(f\"  Median: ${np.median(per_window_mae):.2f}\")\n",
    "    print(f\"  Std:    ${np.std(per_window_mae):.2f}\")\n",
    "    print(f\"{'='*70}\\\\n\")\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    return actuals_returns, predictions_returns, actuals_prices, predictions_prices, target_scaler\n",
    "############################## 8. ##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d97b4d-e644-4e22-91b6-d1c30c605116",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## 9(Scalable).CORR Direction Visualization #######################\n",
    "def plot_direction_analysis_CORR(actuals, predictions, horizon=None):\n",
    "    \"\"\"\n",
    "    Visual analysis of directional accuracy for H-day horizon\n",
    "    Args:\n",
    "        actuals_prices: (n_windows, horizon) array of actual prices in $\n",
    "        predictions_prices: (n_windows, horizon) array of predicted prices in $\n",
    "        horizon: Forecast horizon (auto-detected if None)\n",
    "    \"\"\"\n",
    "    \n",
    "    if horizon is None:\n",
    "        horizon = actuals.shape[1]  # From data shape\n",
    "    \n",
    "    # Transform to original prices\n",
    "    actuals_flat = actuals.flatten()\n",
    "    predictions_flat = predictions.flatten()\n",
    "    \n",
    "    actuals_original = target_scaler.inverse_transform(\n",
    "        actuals_flat.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    predictions_original = target_scaler.inverse_transform(\n",
    "        predictions_flat.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    n_windows = len(actuals)\n",
    "    actuals_prices = actuals_original.reshape(n_windows, horizon)\n",
    "    predicted_prices = predictions_original.reshape(n_windows, horizon)\n",
    "    \n",
    "    # Calculate changes using horizon-1\n",
    "    actual_changes = actuals_prices[:, horizon-1] - actuals_prices[:, 0]\n",
    "    pred_changes = predicted_prices[:, horizon-1] - predicted_prices[:, 0]\n",
    "    \n",
    "    # Plot setup\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # ==========================================\n",
    "    # Subplot 1: Scatter of changes\n",
    "    # ==========================================\n",
    "    axes[0, 0].scatter(actual_changes, pred_changes, alpha=0.5)\n",
    "    \n",
    "    max_change = max(abs(actual_changes.max()), abs(actual_changes.min()), \n",
    "                     abs(pred_changes.max()), abs(pred_changes.min()))\n",
    "    limit = max_change * 1.1\n",
    "    \n",
    "    axes[0, 0].plot([-limit, limit], [-limit, limit], 'r--', label='Perfect Prediction')\n",
    "    axes[0, 0].axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[0, 0].axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[0, 0].set_xlabel(f'Actual Price Change ($) - {horizon} Days')\n",
    "    axes[0, 0].set_ylabel(f'Predicted Price Change ($) - {horizon} Days')\n",
    "    axes[0, 0].set_title(f'Predicted vs Actual Changes ({horizon}-Day)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ==========================================\n",
    "    # Subplot 2: Histogram of changes\n",
    "    # ==========================================\n",
    "    axes[0, 1].hist(actual_changes, bins=50, alpha=0.5, label='Actual', color='blue')\n",
    "    axes[0, 1].hist(pred_changes, bins=50, alpha=0.5, label='Predicted', color='orange')\n",
    "    axes[0, 1].axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[0, 1].set_xlabel(f'Price Change ($) - {horizon} Days')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title(f'Distribution of Price Changes ({horizon}-Day)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ==========================================\n",
    "    # Subplot 3: Rolling direction accuracy\n",
    "    # ==========================================\n",
    "    actual_direction = actual_changes > 0\n",
    "    pred_direction = pred_changes > 0\n",
    "    direction_correct = (actual_direction == pred_direction)\n",
    "    \n",
    "    window_size = max(horizon // 3, 10)  # Dynamic window size\n",
    "    if len(direction_correct) > window_size:\n",
    "        rolling_accuracy = np.convolve(\n",
    "            direction_correct.astype(float), \n",
    "            np.ones(window_size)/window_size, \n",
    "            mode='valid'\n",
    "        ) * 100\n",
    "    else:\n",
    "        rolling_accuracy = [np.mean(direction_correct) * 100]\n",
    "    \n",
    "    axes[0, 2].plot(rolling_accuracy)\n",
    "    axes[0, 2].axhline(50, color='red', linestyle='--', label='Random (50%)')\n",
    "    axes[0, 2].set_xlabel('Window')\n",
    "    axes[0, 2].set_ylabel('Direction Accuracy (%)')\n",
    "    axes[0, 2].set_title(f'Rolling Direction Accuracy - {horizon} Day (window={window_size})')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].set_ylim([0, 100])\n",
    "    \n",
    "    # ==========================================\n",
    "    # Subplot 4: Confusion matrix\n",
    "    # ==========================================\n",
    "    # Use horizon-1\n",
    "    actual_direction_overall = actuals_prices[:, horizon-1] > actuals_prices[:, 0]\n",
    "    pred_direction_overall = predicted_prices[:, horizon-1] > predicted_prices[:, 0]\n",
    "    \n",
    "    tp = ((actual_direction_overall) & (pred_direction_overall)).sum()\n",
    "    tn = ((~actual_direction_overall) & (~pred_direction_overall)).sum()\n",
    "    fp = ((~actual_direction_overall) & (pred_direction_overall)).sum()\n",
    "    fn = ((actual_direction_overall) & (~pred_direction_overall)).sum()\n",
    "    \n",
    "    confusion = np.array([[tn, fp], [fn, tp]])\n",
    "    \n",
    "    im = axes[1, 0].imshow(confusion, cmap='Blues')\n",
    "    axes[1, 0].set_xticks([0, 1])\n",
    "    axes[1, 0].set_yticks([0, 1])\n",
    "    axes[1, 0].set_xticklabels(['Pred DOWN', 'Pred UP'])\n",
    "    axes[1, 0].set_yticklabels(['Actual DOWN', 'Actual UP'])\n",
    "    axes[1, 0].set_title(f'Confusion Matrix ({horizon}-Day Direction)')\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            text = axes[1, 0].text(j, i, confusion[i, j],\n",
    "                                  ha=\"center\", va=\"center\", color=\"black\", fontsize=16)\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[1, 0])\n",
    "    \n",
    "    # ==========================================\n",
    "    # Subplot 5 - Dynamic weekly accuracy\n",
    "    # ==========================================\n",
    "    weekly_accuracies = []\n",
    "    week_labels = []\n",
    "    \n",
    "    # Generate checkpoints every 5 days up to horizon\n",
    "    week_checkpoints = list(range(4, horizon, 5))  # [4, 9, 14, ...]\n",
    "    \n",
    "    for i, day_idx in enumerate(week_checkpoints[:10]):  # Max 10 weeks for readability\n",
    "        if day_idx >= horizon:\n",
    "            break\n",
    "            \n",
    "        actual_dir = actuals_prices[:, day_idx] > actuals_prices[:, 0]\n",
    "        pred_dir = predicted_prices[:, day_idx] > predicted_prices[:, 0]\n",
    "        accuracy = np.mean(actual_dir == pred_dir) * 100\n",
    "        weekly_accuracies.append(accuracy)\n",
    "        week_labels.append(f'Week {i+1}\\n(Day {day_idx+1})')\n",
    "    \n",
    "    if weekly_accuracies:\n",
    "        axes[1, 1].bar(range(len(weekly_accuracies)), weekly_accuracies, \n",
    "                      color='steelblue', alpha=0.7)\n",
    "        axes[1, 1].axhline(50, color='red', linestyle='--', label='Random (50%)')\n",
    "        axes[1, 1].set_xlabel('Week')\n",
    "        axes[1, 1].set_ylabel('Direction Accuracy (%)')\n",
    "        axes[1, 1].set_title('Weekly Direction Accuracy')\n",
    "        axes[1, 1].set_xticks(range(len(weekly_accuracies)))\n",
    "        axes[1, 1].set_xticklabels(week_labels, fontsize=9)\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "        axes[1, 1].set_ylim([0, 100])\n",
    "    \n",
    "    # ==========================================\n",
    "    # Subplot 6 - Magnitude comparison\n",
    "    # ==========================================\n",
    "    correct_idx = (actual_direction == pred_direction)\n",
    "    \n",
    "    if correct_idx.sum() > 0:\n",
    "        avg_actual = np.mean(np.abs(actual_changes[correct_idx]))\n",
    "        avg_pred = np.mean(np.abs(pred_changes[correct_idx]))\n",
    "        \n",
    "        axes[1, 2].bar(['Actual', 'Predicted'], [avg_actual, avg_pred], \n",
    "                      color=['blue', 'orange'], alpha=0.7)\n",
    "        axes[1, 2].set_ylabel('Average Magnitude ($)')\n",
    "        axes[1, 2].set_title(f'Avg {horizon}-Day Change Magnitude\\n(When Direction Correct)')\n",
    "        axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        ratio = avg_pred / avg_actual if avg_actual > 0 else 0\n",
    "        axes[1, 2].text(0.5, max(avg_actual, avg_pred) * 0.9, \n",
    "                       f'Ratio: {ratio:.2f}',\n",
    "                       ha='center', fontsize=12, \n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "##################################### 9.CORR (Scalable) ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8baeef4-fe7b-4c82-832e-a5b270fa3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 10 (Scalable) Weighted Directional Loss ############################\n",
    "#@tf.function  # Compile to graph for faster execution\n",
    "def directional_mse_loss_weighted_OPT(alpha=20.0, beta=3.0):\n",
    "    \"\"\"\n",
    "    Penalizes directional errors MORE when the true move is large\n",
    "    This makes the model prioritize getting big moves right\n",
    "    Args:\n",
    "        alpha: Base directional penalty weight\n",
    "        beta: Magnitude scaling factor (higher = more weight on large moves)\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        # Standard MSE\n",
    "        mse = K.mean(K.square(y_true - y_pred))\n",
    "        \n",
    "        # Cumulative returns\n",
    "        true_cumulative = K.sum(y_true, axis=1, keepdims=True)\n",
    "        pred_cumulative = K.sum(y_pred, axis=1, keepdims=True)\n",
    "        \n",
    "        # Directions\n",
    "        true_direction = K.sign(true_cumulative)\n",
    "        pred_direction = K.sign(pred_cumulative)\n",
    "        \n",
    "        # Direction match\n",
    "        direction_match = K.cast(\n",
    "            K.equal(true_direction, pred_direction), \n",
    "            dtype='float32'\n",
    "        )\n",
    "        \n",
    "        # Weight by magnitude of true move\n",
    "        magnitude_weight = K.abs(true_cumulative) * beta\n",
    "        \n",
    "        # Weighted directional loss\n",
    "        weighted_direction_loss = K.mean(\n",
    "            (1.0 - direction_match) * (1.0 + magnitude_weight)\n",
    "        )\n",
    "        \n",
    "        total_loss = mse + alpha * weighted_direction_loss\n",
    "        \n",
    "        return total_loss\n",
    "    return loss\n",
    "##################################### 10. (Scalable) ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ad1aa-2a0a-4395-b90e-3fa191b05720",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 10.1 (Scalable) Clip Weighted Directional Loss ######################\n",
    "def directional_mse_loss_clip_weighted(alpha=10.0, beta=1.0, max_penalty=5.0):\n",
    "    \"\"\"\n",
    "    Add clipping to prevent huge losses\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        mse = K.mean(K.square(y_true - y_pred))\n",
    "        \n",
    "        true_cumulative = K.sum(y_true, axis=1, keepdims=True)\n",
    "        pred_cumulative = K.sum(y_pred, axis=1, keepdims=True)\n",
    "        \n",
    "        true_direction = K.sign(true_cumulative)\n",
    "        pred_direction = K.sign(pred_cumulative)\n",
    "        \n",
    "        direction_match = K.cast(\n",
    "            K.equal(true_direction, pred_direction),\n",
    "            K.floatx()\n",
    "        )\n",
    "        \n",
    "        # CLIP magnitude weight to prevent explosion\n",
    "        magnitude_weight = K.clip(K.abs(true_cumulative) * beta, 0.0, max_penalty)\n",
    "        \n",
    "        weighted_direction_loss = K.mean(\n",
    "            (1.0 - direction_match) * (1.0 + magnitude_weight)\n",
    "        )\n",
    "        \n",
    "        # CLIP total directional component\n",
    "        total_loss = mse + K.clip(alpha * weighted_direction_loss, 0.0, max_penalty)\n",
    "        \n",
    "        return total_loss\n",
    "    return loss\n",
    "############################## 10.1 #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d48b1f-9f31-422d-84fe-e658c33df06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Check correlation bitween continuous features #######################\n",
    "def correlation_fun(data, features_no_close=None):\n",
    "    threshold = 0.85\n",
    "    data_df = data.copy()\n",
    "    if features_no_close:\n",
    "        correlation_matrix = data_df[features_no_close].corr()\n",
    "    else:\n",
    "        correlation_matrix = data_df.corr()\n",
    "        \n",
    "    # Identify highly correlated pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'feature1': correlation_matrix.columns[i],\n",
    "                    'feature2': correlation_matrix.columns[j],\n",
    "                    'correlation': correlation_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    print(\"High correlation pairs:\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"{pair['feature1']} <-> {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad7e57-2f7f-4e15-8ff1-2545d9ce35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "def generate_random_hyperparams_OPT(seed=None):\n",
    "    \"\"\"\n",
    "    Generate random hyperparameter combinations\n",
    "    Uses random search across predefined ranges\n",
    "    Args:\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        dict: Hyperparameter configuration\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "   \n",
    "    # LatinHypercube-inspired sampling\n",
    "    lstm_units_options = [16, 32, 64, 128]\n",
    "    dropout_options = np.linspace(0.1, 0.4, 10)\n",
    "    lr_options = np.logspace(-5, -3, 20)  # log scale for learning rate\n",
    "    epochs_options = range(10, 35, 2)\n",
    "    batch_size_options = [16, 32, 64]\n",
    "    loss_type_options = ['mse', 'clip']   # 'conservative' (weighted), 'aggressive' (weighted)\n",
    "\n",
    "\n",
    "    hp = {\n",
    "        'lstm_units': np.random.choice(lstm_units_options),\n",
    "        'dropout': np.random.choice(dropout_options),\n",
    "        'lr': np.random.choice(lr_options),\n",
    "        'epochs': np.random.choice(epochs_options),\n",
    "        'batch_size': np.random.choice(batch_size_options),\n",
    "        'loss_type': np.random.choice(loss_type_options)\n",
    "    }\n",
    "    \n",
    "    return hp\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b81e2-7724-4adb-8b9b-3a131c0fb5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################  Latin Hypercube  ##############################################\n",
    "from scipy.stats import qmc\n",
    "def generate_hyperparams_LHS(n_samples=10, seed=None):\n",
    "    \"\"\"\n",
    "    Generate hyperparameters using Latin Hypercube Sampling.\n",
    "    LHS ensures better coverage of the search space compared to random sampling.\n",
    "    Each parameter is divided into n_samples bins, and exactly one sample is\n",
    "    drawn from each bin (with random permutation across parameters).\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of hyperparameter sets to generate\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        list of dicts: n_samples hyperparameter configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Create LHS sampler (5 dimensions = 5 hyperparameters)\n",
    "    sampler = qmc.LatinHypercube(d=5, seed=seed)\n",
    "    \n",
    "    # Generate samples in [0, 1]^5 hypercube\n",
    "    samples = sampler.random(n=n_samples)\n",
    "    \n",
    "    # Define parameter ranges\n",
    "    lstm_units_options = [32, 64]\n",
    "    dropout_range = [0.1, 0.4]\n",
    "    lr_range_log = [-4, -3]  # log10 scale\n",
    "    epochs_range = [10, 34]\n",
    "    batch_size_options = [16, 32, 64]\n",
    "    #loss_type_options = ['mse']   # 'conservative' (weighted), 'aggressive' (weighted),  'clip' (weighted)\n",
    "    \n",
    "    # Initialize list to store all hyperparameter sets\n",
    "    hp_list = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Extract sample for this trial\n",
    "        sample = samples[i]\n",
    "        \n",
    "        # 1. LSTM units (discrete: map to 4 options)\n",
    "        lstm_idx = int(sample[0] * len(lstm_units_options))\n",
    "        lstm_idx = min(lstm_idx, len(lstm_units_options) - 1)  # Safety\n",
    "        lstm_units = lstm_units_options[lstm_idx]\n",
    "        \n",
    "        # 2. Dropout (continuous: scale to [0.1, 0.4])\n",
    "        dropout = dropout_range[0] + sample[1] * (dropout_range[1] - dropout_range[0])\n",
    "        \n",
    "        # 3. Learning rate (log scale: map to [1e-5, 1e-3])\n",
    "        lr_log = lr_range_log[0] + sample[2] * (lr_range_log[1] - lr_range_log[0])\n",
    "        lr = 10 ** lr_log\n",
    "        \n",
    "        # 4. Epochs (discrete: map to [10, 34])\n",
    "        epochs = int(epochs_range[0] + sample[3] * (epochs_range[1] - epochs_range[0] + 1))\n",
    "        epochs = min(epochs, epochs_range[1])  # Safety\n",
    "        \n",
    "        # 5. Batch size (discrete: map to 3 options)\n",
    "        batch_idx = int(sample[4] * len(batch_size_options))\n",
    "        batch_idx = min(batch_idx, len(batch_size_options) - 1)  # Safety\n",
    "        batch_size = batch_size_options[batch_idx]\n",
    "\n",
    "        #loss_type = np.random.choice(loss_type_options)   \n",
    "        loss_type = 'mse'   #'loss_type': 'mse'  # Based on testing\n",
    "        \n",
    "        # Create hyperparameter dict\n",
    "        hp = {\n",
    "            'lstm_units': int(lstm_units),\n",
    "            'dropout': float(dropout),\n",
    "            'lr': float(lr),\n",
    "            'epochs': int(epochs),\n",
    "            'batch_size': int(batch_size),\n",
    "            'loss_type': loss_type\n",
    "        }\n",
    "        \n",
    "        hp_list.append(hp)\n",
    "    \n",
    "    return hp_list\n",
    "#################### Latin Hypercube #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ead492-2588-4361-91af-03a594cd9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Build LSTM Model Function (accepts hyperparameters:) (Scalable) ########\n",
    "def build_lstm_model_OPT(time_step, num_features, horizon,\n",
    "                     lstm_units=64, dropout_rate=0.2, learning_rate=1e-3, loss_type='mse'):\n",
    "    \"\"\"\n",
    "      Build LSTM model for multi-step returns forecasting\n",
    "      Architecture: LSTM → LSTM → Dense\n",
    "      Features: Dropout, gradient clipping, custom loss options\n",
    "      Args:\n",
    "          time_step: Lookback window length\n",
    "          num_features: Number of input features\n",
    "          horizon: Forecast horizon length\n",
    "          lstm_units: Units per LSTM layer (default: 64)\n",
    "          dropout_rate: Dropout rate (default: 0.2)\n",
    "          learning_rate: Adam learning rate (default: 1e-3)\n",
    "          loss_type: Loss function type\n",
    "              - 'mse': Standard MSE  (default)\n",
    "              - 'conservative': Directional loss (a=10, b=1)\n",
    "              - 'aggressive': Directional loss (a=50, b=5)\n",
    "              - 'clip': Clipped directional loss \n",
    "      \n",
    "      Returns:\n",
    "          Compiled Keras Sequential model\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================\n",
    "    # Use directional loss\n",
    "    # ==========================================\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(time_step, num_features)))\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, dropout=dropout_rate))\n",
    "    model.add(LSTM(lstm_units, return_sequences=False, dropout=dropout_rate))\n",
    "    model.add(Dense(horizon))  # This automatically handles horizon\n",
    "    optimizer = Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        clipnorm=1.0                      # Gradient clipping. Prevent exploding gradients\n",
    "    )\n",
    "    \n",
    "    # Select loss function\n",
    "    if loss_type == 'conservative':\n",
    "        custom_loss = directional_mse_loss_weighted_OPT(\n",
    "            alpha=10.0, beta=1.0\n",
    "        )\n",
    "    elif loss_type == 'aggressive':\n",
    "        custom_loss = directional_mse_loss_weighted_OPT(\n",
    "            alpha=50.0, beta=5.0\n",
    "        )\n",
    "    elif loss_type == 'mse':\n",
    "        custom_loss = 'mse'\n",
    "    \n",
    "    elif loss_type == 'clip':\n",
    "        custom_loss = directional_mse_loss_clip_weighted(\n",
    "            alpha=10.0, beta=1.0, max_penalty=3.0\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "#################################### (Scalable) #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7380988-6606-4da8-b125-6b693c1c35e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Analyze_direction_accuracy #####################################\n",
    "def analyze_direction_accuracy_CORRECTED(actuals_prices, predictions_prices, \n",
    "                                          horizon=None):\n",
    "    \"\"\"\n",
    "    Analyze directional accuracy using ALREADY RECONSTRUCTED prices\n",
    "    \n",
    "    Args:\n",
    "        actuals_prices: (n_windows, horizon) array of actual prices in $\n",
    "        predictions_prices: (n_windows, horizon) array of predicted prices in $\n",
    "        horizon: forecast horizon (inferred from data if None)\n",
    "    \n",
    "    Returns:\n",
    "        actual_changes, pred_changes, statistics_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    if horizon is None:\n",
    "        horizon = actuals_prices.shape[1]\n",
    "    \n",
    "    n_windows = len(actuals_prices)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"DIRECTIONAL ACCURACY ANALYSIS ({horizon}-DAY HORIZON)\") \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. Overall direction (Day 1 → Day H)\n",
    "    # ==========================================\n",
    "    print(f\"\\n1. OVERALL DIRECTION (Day 1 → Day {horizon}):\")  \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Calculate actual price changes\n",
    "    actual_changes = actuals_prices[:, -1] - actuals_prices[:, 0]\n",
    "    pred_changes = predictions_prices[:, -1] - predictions_prices[:, 0]\n",
    "    \n",
    "    # Direction based on changes\n",
    "    actual_direction_overall = actual_changes > 0\n",
    "    pred_direction_overall = pred_changes > 0\n",
    "    \n",
    "    direction_match_overall = (actual_direction_overall == pred_direction_overall)\n",
    "    accuracy_overall = np.mean(direction_match_overall) * 100\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_overall:.2f}%\")\n",
    "    print(f\"Correct: {direction_match_overall.sum()}/{len(direction_match_overall)}\")\n",
    "    \n",
    "    # Print sample changes for verification\n",
    "    print(f\"\\nSample changes (first 3 windows):\")\n",
    "    for i in range(min(3, n_windows)):\n",
    "        print(f\"  Window {i}: Actual=${actual_changes[i]:.2f}, \"\n",
    "              f\"Pred=${pred_changes[i]:.2f}, \"\n",
    "              f\"Match={'✓' if direction_match_overall[i] else '✗'}\")\n",
    "    \n",
    "    # ... rest of analysis (weekly, day-to-day, etc.) ...\n",
    "    # ==========================================\n",
    "    # 2. Weekly directions (every 5 days)\n",
    "    # ==========================================\n",
    "    print(\"\\n2. WEEKLY DIRECTIONS (5-day intervals):\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Generate week checkpoints dynamically\n",
    "    week_checkpoints = list(range(4, horizon, 5))  # [4, 9, 14, 19, 24, ...]\n",
    "    weekly_accuracies = []\n",
    "    \n",
    "    for i, day_idx in enumerate(week_checkpoints):\n",
    "        if day_idx >= horizon:  # ← Safety check\n",
    "            break\n",
    "            \n",
    "        actual_dir = actuals_prices[:, day_idx] > actuals_prices[:, 0]\n",
    "        pred_dir = predictions_prices[:, day_idx] > predictions_prices[:, 0]\n",
    "        match = (actual_dir == pred_dir)\n",
    "        acc = np.mean(match) * 100\n",
    "        weekly_accuracies.append(acc)\n",
    "        \n",
    "        week_num = i + 1\n",
    "        print(f\"Week {week_num} (Day {day_idx+1}): {acc:.2f}%\")\n",
    "    \n",
    "    if weekly_accuracies:\n",
    "        print(f\"\\nAverage weekly accuracy: {np.mean(weekly_accuracies):.2f}%\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. Day-by-day directions (all transitions)\n",
    "    # ==========================================\n",
    "    print(\"\\n3. DAY-TO-DAY DIRECTIONS (summary by week):\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Generate week ranges dynamically\n",
    "    week_ranges = [(i*5, min((i+1)*5, horizon)) for i in range((horizon + 4) // 5)]\n",
    "    \n",
    "    for week_num, (start, end) in enumerate(week_ranges, 1):\n",
    "        if start >= horizon - 1:\n",
    "            break\n",
    "            \n",
    "        week_accs = []\n",
    "        for i in range(start, min(end-1, horizon-1)):\n",
    "            actual_dir = actuals_prices[:, i+1] > actuals_prices[:, i]\n",
    "            pred_dir = predictions_prices[:, i+1] > predictions_prices[:, i]\n",
    "            match = (actual_dir == pred_dir)\n",
    "            acc = np.mean(match) * 100\n",
    "            week_accs.append(acc)\n",
    "        \n",
    "        if week_accs:\n",
    "            avg_week_acc = np.mean(week_accs)\n",
    "            print(f\"Week {week_num} avg (days {start+1}-{min(end, horizon)}): {avg_week_acc:.2f}%\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 4. Magnitude Analysis (H-day cumulative)\n",
    "    # ==========================================\n",
    "    print(f\"\\n4. MAGNITUDE ANALYSIS ({horizon}-Day Cumulative):\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Use horizon-1 instead of hard-coded 44\n",
    "    actual_changes = actuals_prices[:, horizon-1] - actuals_prices[:, 0]\n",
    "    pred_changes = predictions_prices[:, horizon-1] - predictions_prices[:, 0]\n",
    "    \n",
    "    correct_idx = direction_match_overall\n",
    "    \n",
    "    if correct_idx.sum() > 0:\n",
    "        avg_actual_change = np.mean(np.abs(actual_changes[correct_idx]))\n",
    "        avg_pred_change = np.mean(np.abs(pred_changes[correct_idx]))\n",
    "        \n",
    "        print(f\"Avg actual {horizon}-day change (when correct):    ${avg_actual_change:.2f}\")\n",
    "        print(f\"Avg predicted {horizon}-day change (when correct): ${avg_pred_change:.2f}\")\n",
    "        \n",
    "        if avg_actual_change > 0:\n",
    "            print(f\"Ratio (pred/actual): {avg_pred_change/avg_actual_change:.2f}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 5. Detailed Breakdown\n",
    "    # ==========================================\n",
    "    print(f\"\\n5. DETAILED BREAKDOWN (Overall {horizon}-Day Direction):\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    tp = ((actual_direction_overall) & (pred_direction_overall)).sum()\n",
    "    tn = ((~actual_direction_overall) & (~pred_direction_overall)).sum()\n",
    "    fp = ((~actual_direction_overall) & (pred_direction_overall)).sum()\n",
    "    fn = ((actual_direction_overall) & (~pred_direction_overall)).sum()\n",
    "    \n",
    "    print(f\"True Positives (predicted UP, was UP):     {tp}\")\n",
    "    print(f\"True Negatives (predicted DOWN, was DOWN): {tn}\")\n",
    "    print(f\"False Positives (predicted UP, was DOWN):  {fp}\")\n",
    "    print(f\"False Negatives (predicted DOWN, was UP):  {fn}\")\n",
    "    \n",
    "    if (tp + fp) > 0:\n",
    "        precision = tp / (tp + fp)\n",
    "        print(f\"\\nPrecision (UP predictions): {precision:.2%}\")\n",
    "    \n",
    "    if (tp + fn) > 0:\n",
    "        recall = tp / (tp + fn)\n",
    "        print(f\"Recall (catching UP moves): {recall:.2%}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 6. Direction by Time Period\n",
    "    # ==========================================\n",
    "    print(\"\\n6. DIRECTIONAL ACCURACY OVER TIME:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    quarter_size = n_windows // 4\n",
    "    \n",
    "    for q in range(4):\n",
    "        start_idx = q * quarter_size\n",
    "        end_idx = (q + 1) * quarter_size if q < 3 else n_windows\n",
    "        \n",
    "        q_matches = direction_match_overall[start_idx:end_idx]\n",
    "        q_accuracy = np.mean(q_matches) * 100\n",
    "        \n",
    "        print(f\"Quarter {q+1} (windows {start_idx:3d}-{end_idx:3d}): {q_accuracy:5.2f}%\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # SUMMARY\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Overall {horizon}-Day Direction Accuracy: {accuracy_overall:.2f}%\")\n",
    "    \n",
    "    if accuracy_overall > 65:\n",
    "        print(f\" EXCELLENT - Model captures {horizon}-day trends very well!\")\n",
    "    elif accuracy_overall > 60:\n",
    "        print(f\" GOOD - Model has solid {horizon}-day directional edge\")\n",
    "    elif accuracy_overall > 55:\n",
    "        print(f\" MODERATE - Better than random, shows promise\")\n",
    "    else:\n",
    "        print(f\" POOR - Not significantly better than random\")\n",
    "    \n",
    "    return actual_changes, pred_changes, {\n",
    "        'accuracy': accuracy_overall,\n",
    "        'weekly_accuracies': weekly_accuracies,\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "        'correct_predictions': direction_match_overall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1639f-90ed-4ec6-9320-5fb9b8b30e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Add OHLC featres #########################################\n",
    "def add_ohlc_features_CORR(data_df):\n",
    "    \"\"\"\n",
    "    Adds 13 candlestick/OHLC-based features to capture intraday dynamics\n",
    "    \"\"\"\n",
    "    df = data_df.copy()\n",
    "    # ==========================================\n",
    "    # 1. Basic OHLC returns (stationary)\n",
    "    # ==========================================\n",
    "    df['Open_return'] = df['Open'].pct_change()\n",
    "    df['High_return'] = df['High'].pct_change()\n",
    "    df['Low_return'] = df['Low'].pct_change()\n",
    "    # Close_return already exists\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. Gap Analysis (overnight moves)\n",
    "    # ==========================================\n",
    "    df['gap'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1)\n",
    "    # Gap down filled: gap > 0 and price recovered\n",
    "    gap_down_filled = (df['gap'] > 0) & (df['Low'] < df['Open'])\n",
    "    # Gap up filled: gap < 0 and price declined  \n",
    "    gap_up_filled = (df['gap'] < 0) & (df['High'] > df['Open'])\n",
    "    # Either gap filled\n",
    "    df['gap_filled'] = (gap_down_filled | gap_up_filled).astype(int)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. Intraday Range (volatility proxy)\n",
    "    # ==========================================\n",
    "    df['daily_range'] = (df['High'] - df['Low']) / df['Close']\n",
    "    df['range_pct_change'] = df['daily_range'].pct_change()\n",
    "    \n",
    "    # ==========================================\n",
    "    # 4. Close Position in Range (strength)\n",
    "    # ==========================================\n",
    "    # 0 = closed at low, 1 = closed at high\n",
    "    df['close_position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'] + 1e-10)  # Avoid div by 0\n",
    "    \n",
    "    # ==========================================\n",
    "    # 5. Body and Wicks (candlestick analysis)\n",
    "    # ==========================================\n",
    "    df['body'] = (df['Close'] - df['Open']) / df['Close']  # Positive = bullish candle\n",
    "    df['upper_wick'] = (df['High'] - df[['Open', 'Close']].max(axis=1)) / df['Close']\n",
    "    df['lower_wick'] = (df[['Open', 'Close']].min(axis=1) - df['Low']) / df['Close']\n",
    "    \n",
    "    # ==========================================\n",
    "    # 6. OHLC Relative Relationships\n",
    "    # ==========================================\n",
    "    df['high_close_ratio'] = df['High'] / df['Close']  # How much higher was intraday peak?\n",
    "    df['low_close_ratio'] = df['Low'] / df['Close']    # How much lower was intraday trough?\n",
    "\n",
    "    # ==========================================\n",
    "    # Final cleanup\n",
    "    # ==========================================    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a02b0f-6622-4b72-b30c-4bbecd95d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Add Macro Regime Featurres ##################################\n",
    "def add_macro_regime_features_CORR(data_df, vix_threshold=20,\n",
    "                                         curve_normal=0.5,\n",
    "                                         curve_steep=1.5,\n",
    "                                         rolling_window=60,\n",
    "                                         min_periods=30):\n",
    "    \"\"\"\n",
    "    Add regime based on FUNDAMENTAL indicators\n",
    "    Not technical indicators!\n",
    "    Returns: DataFrame with 4 additional columns\n",
    "     \n",
    "     Missing values are forward-filled then filled with 0\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    required_cols = ['DGS10', 'VIX_indx_close', 'T10Y2Y', 'gold_vix']\n",
    "    missing = [col for col in required_cols if col not in data_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    \n",
    "    df = data_df.copy()\n",
    "    \n",
    "    # Economic regime (based on fundamental features)\n",
    "    # 1. Interest rate regime(with NaN handling)\n",
    "    rate_rolling_median = (\n",
    "        df['DGS10']\n",
    "        .rolling(rolling_window, min_periods=min_periods)\n",
    "        .median()\n",
    "        .shift(1)  # Avoid data leakage\n",
    "     #   .fillna(method='bfill')  # Fill initial NaN\n",
    "    )\n",
    "    df['rate_regime'] = (df['DGS10'] > rate_rolling_median).astype(int)\n",
    "    \n",
    "    # 2. Volatility regime (VIX-based simple threshold)\n",
    "    # Low vol - 0; High vol - 1\n",
    "    df['vol_regime'] = (df['VIX_indx_close'] > vix_threshold).astype(int)\n",
    "    \n",
    "    # 3. Yield curve regime(categorical)\n",
    "    df['curve_regime'] = 0  # Flat/inverted\n",
    "    df.loc[df['T10Y2Y'] > curve_normal, 'curve_regime'] = 1  # Normal\n",
    "    df.loc[df['T10Y2Y'] > curve_steep, 'curve_regime'] = 2  # Steep\n",
    "    \n",
    "    # 4. Gold/Dollar regime (risk on/off with NaN handling)\n",
    "    risk_rolling_median = (\n",
    "        df['gold_vix']\n",
    "        .rolling(rolling_window, min_periods=min_periods)\n",
    "        .median()\n",
    "        .shift(1)  # Avoid data leakage\n",
    "        #.fillna(method='bfill')  # Fill initial NaN\n",
    "    )\n",
    "    df['risk_regime'] = (df['gold_vix'] > risk_rolling_median).astype(int)\n",
    "    df['risk_regime'] = (df['gold_vix'] > df['gold_vix'].rolling(60).median().shift(1)).astype(int)\n",
    "\n",
    "    return df\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935294ef-9b53-4e15-8a3e-07fdb9f996b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Add TA featres ####################################\n",
    "def add_technical_momentum_features_CORR(data_df):\n",
    "    \"\"\"\n",
    "    Add technical indicators for short-term (5-15 day) predictions\n",
    "    adds 19 TA features, loses ~50 rows\n",
    "    \"\"\"\n",
    "    df = data_df.copy()\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. RSI (momentum)\n",
    "    # ==========================================\n",
    "    from ta.momentum import RSIIndicator\n",
    "    df['RSI_14'] = RSIIndicator(df['Close'], window=14).rsi()\n",
    "    df['RSI_7'] = RSIIndicator(df['Close'], window=7).rsi()\n",
    "    \n",
    "    # ADDED: Normalize RSI to [-1, 1] range for better scaling\n",
    "    df['RSI_14_norm'] = (df['RSI_14'] - 50) / 50  # -1 (oversold) to +1 (overbought)\n",
    "    df['RSI_7_norm'] = (df['RSI_7'] - 50) / 50\n",
    "    \n",
    "    # Drop raw RSI (keep normalized versions)\n",
    "    df = df.drop(columns=['RSI_14', 'RSI_7'])\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. MACD (trend)\n",
    "    # ==========================================\n",
    "    from ta.trend import MACD\n",
    "    macd = MACD(df['Close'])\n",
    "    df['MACD'] = macd.macd()\n",
    "    df['MACD_signal'] = macd.macd_signal()\n",
    "    df['MACD_diff'] = macd.macd_diff()\n",
    "    \n",
    "    # ADDED: Normalize MACD by price (make it scale-invariant)\n",
    "    df['MACD_norm'] = df['MACD'] / df['Close']\n",
    "    df['MACD_signal_norm'] = df['MACD_signal'] / df['Close']\n",
    "    df['MACD_diff_norm'] = df['MACD_diff'] / df['Close']\n",
    "    \n",
    "    # Drop raw MACD (keep normalized versions)\n",
    "    df = df.drop(columns=['MACD', 'MACD_signal', 'MACD_diff'])\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. Bollinger Bands (volatility + mean reversion)\n",
    "    # ==========================================\n",
    "    from ta.volatility import BollingerBands\n",
    "    bb = BollingerBands(df['Close'])\n",
    "    df['BB_width'] = (bb.bollinger_hband() - bb.bollinger_lband()) / df['Close']\n",
    "    df['BB_position'] = (df['Close'] - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())\n",
    "    \n",
    "    # BB squeeze indicator (low volatility before breakout)\n",
    "    df['BB_squeeze'] = (df['BB_width'] < df['BB_width'].rolling(20).mean()).astype(int)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 4. EMA - SMA (Moving Averages)\n",
    "    # ==========================================\n",
    "    # Short-term: EMA (minimal data loss)\n",
    "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "    \n",
    "    # Long-term: SMA (trend following)\n",
    "    df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "    df['SMA_50'] = df['Close'].rolling(50).mean()\n",
    "    \n",
    "    # Derived features (relative to price)\n",
    "    df['price_vs_EMA10'] = (df['Close'] - df['EMA_10']) / df['Close']\n",
    "    df['price_vs_EMA20'] = (df['Close'] - df['EMA_20']) / df['Close']\n",
    "    df['price_vs_SMA20'] = (df['Close'] - df['SMA_20']) / df['Close'] \n",
    "    df['price_vs_SMA50'] = (df['Close'] - df['SMA_50']) / df['Close']\n",
    "    \n",
    "    # Crossover signals (binary)\n",
    "    df['EMA_cross'] = (df['EMA_10'] > df['EMA_20']).astype(int)\n",
    "    df['MA_cross'] = (df['SMA_20'] > df['SMA_50']).astype(int)\n",
    "    \n",
    "    # Normalize trend_strength by price\n",
    "    df['trend_strength_ema'] = (df['EMA_20'] - df['SMA_50']) / df['Close']\n",
    "    \n",
    "    # Drop raw MA columns (save memory)\n",
    "    df = df.drop(columns=['EMA_10', 'EMA_20', 'SMA_20', 'SMA_50'])\n",
    "    \n",
    "    # ==========================================\n",
    "    # 5. Volume momentum\n",
    "    # ==========================================\n",
    "    df['volume_SMA_20'] = df['Volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['Volume'] / (df['volume_SMA_20'] + 1e-10)  # ← ADDED: Avoid div by 0\n",
    "    \n",
    "    # ADDED: Volume trend (is volume increasing or decreasing?)\n",
    "    df['volume_trend'] = df['volume_SMA_20'].pct_change(5)\n",
    "    \n",
    "    # Drop raw volume SMA\n",
    "    df = df.drop(columns=['volume_SMA_20'])\n",
    "    \n",
    "    # ==========================================\n",
    "    # 6. Additional Momentum Indicators\n",
    "    # ==========================================\n",
    "    \n",
    "    # ADDED: Rate of Change (momentum)\n",
    "    df['ROC_10'] = ((df['Close'] - df['Close'].shift(10)) / df['Close'].shift(10))\n",
    "    df['ROC_20'] = ((df['Close'] - df['Close'].shift(20)) / df['Close'].shift(20))\n",
    "    \n",
    "    # ADDED: ADX (trend strength)\n",
    "    from ta.trend import ADXIndicator\n",
    "    adx = ADXIndicator(df['High'], df['Low'], df['Close'], window=14)\n",
    "    df['ADX'] = adx.adx()\n",
    "    df['ADX_norm'] = df['ADX'] / 100  # Normalize to [0, 1]\n",
    "    \n",
    "    # Drop raw ADX\n",
    "    df = df.drop(columns=['ADX'])\n",
    "    # ==========================================\n",
    "    # Final cleanup\n",
    "    # ==========================================\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"   Added {df.shape[1] - data_df.shape[1]} technical features\")\n",
    "    print(f\"   Remaining data: {len(df)} rows (lost {len(data_df) - len(df)} rows)\")\n",
    "    \n",
    "    return df\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05cd20-e774-4aa7-9c5c-27bd46bf074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Market Regime Features (10 new) ####################################\n",
    "def add_market_regime_features_CORR(data_df, lookback_short=20, lookback_long=60):\n",
    "    \"\"\"\n",
    "    Add market regime features:\n",
    "    - Trend: Bullish/Bearish/Flat\n",
    "    - Volatility: High/Low\n",
    "    - Combined regime classification\n",
    "    \n",
    "    Parameters:\n",
    "    lookback_short : int\n",
    "        Short-term lookback for trend (default: 20 days)\n",
    "    lookback_long : int\n",
    "        Long-term lookback for trend (default: 60 days)\n",
    "    Adds 10 regime features, loses ~272 rows\n",
    "    \"\"\"\n",
    "    \n",
    "    df = data_df.copy()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ADDING MARKET REGIME FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. TREND REGIME (Bullish/Bearish/Flat)\n",
    "    # ==========================================\n",
    "    \n",
    "    # Calculate SMAs\n",
    "    df['SMA_short'] = df['Close'].rolling(lookback_short).mean()\n",
    "    df['SMA_long'] = df['Close'].rolling(lookback_long).mean()\n",
    "    \n",
    "    # Trend strength (normalized by price)\n",
    "    df['trend_strength_sma'] = (df['SMA_short'] - df['SMA_long']) / df['Close']\n",
    "    \n",
    "    # Classify trend\n",
    "    trend_threshold = 0.02  # 2% difference = clear trend\n",
    "    df['regime_trend'] = 0  # Flat\n",
    "    df.loc[df['trend_strength_sma'] > trend_threshold, 'regime_trend'] = 1   # Bullish\n",
    "    df.loc[df['trend_strength_sma'] < -trend_threshold, 'regime_trend'] = -1 # Bearish\n",
    "    \n",
    "    # Trend momentum (is trend accelerating?)\n",
    "    df['trend_momentum'] = df['trend_strength_sma'].diff(5)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. VOLATILITY REGIME (High/Low)\n",
    "    # ==========================================\n",
    "    \n",
    "    # Historical volatility (20-day rolling std of returns, annualized)\n",
    "    df['volatility_20d'] = df['Close'].pct_change().rolling(20).std() * np.sqrt(252)\n",
    "    \n",
    "    # Volatility percentile (where is current vol vs last year?)\n",
    "    # Calculate percentile using expanding window\n",
    "    df['vol_rank'] = df['volatility_20d'].rolling(252).rank(pct=True)\n",
    "    df['vol_percentile'] = df['vol_rank']  # Last value is the percentile\n",
    "    \n",
    "    # Classify volatility\n",
    "    df['regime_volatility'] = 0  # Low vol\n",
    "    df.loc[df['vol_percentile'] > 0.7, 'regime_volatility'] = 1  # High vol\n",
    "    df.loc[df['vol_percentile'] > 0.9, 'regime_volatility'] = 2  # Extreme vol\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. COMBINED MARKET REGIME\n",
    "    # ==========================================\n",
    "    \n",
    "    # Create single regime feature (9 possible states)\n",
    "    # Encoding: regime_combined = (regime_trend + 1) * 3 + regime_volatility\n",
    "    # Results in: 0-8 representing all combinations\n",
    "    df['regime_combined'] = (df['regime_trend'] + 1) * 3 + df['regime_volatility']\n",
    "    \n",
    "    # Regime labels for interpretation\n",
    "    regime_names = {\n",
    "        0: 'Bearish_LowVol',    1: 'Bearish_HighVol',    2: 'Bearish_ExtremeVol',\n",
    "        3: 'Flat_LowVol',       4: 'Flat_HighVol',       5: 'Flat_ExtremeVol',\n",
    "        6: 'Bullish_LowVol',    7: 'Bullish_HighVol',    8: 'Bullish_ExtremeVol'\n",
    "    }\n",
    "    \n",
    "   \n",
    "    # ==========================================\n",
    "    # CLEANUP\n",
    "    # ==========================================\n",
    "    \n",
    "    # Drop temporary SMA columns\n",
    "    df = df.drop(columns=['SMA_short', 'SMA_long'], errors='ignore')\n",
    "   \n",
    "    # Drop NaN rows from rolling calculations\n",
    "    df = df.dropna()\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. ADDITIONAL REGIME FEATURES\n",
    "    # ==========================================\n",
    "    \n",
    "    # Regime persistence (how many days in current regime?)\n",
    "    df['regime_duration'] = (\n",
    "        df['regime_combined'] != df['regime_combined'].shift(1)\n",
    "    ).cumsum()\n",
    "    df['regime_duration'] = df.groupby('regime_duration').cumcount() + 1\n",
    "    \n",
    "    # Regime transition indicator (just changed?)\n",
    "    df['regime_transition'] = (\n",
    "        df['regime_combined'] != df['regime_combined'].shift(1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 5. VIX-BASED REGIME (if available)\n",
    "    # ==========================================\n",
    "    \n",
    "    if 'VIX_indx_close' in df.columns:\n",
    "        # VIX regime thresholds\n",
    "        df['regime_vix'] = 0  # Low fear\n",
    "        df.loc[df['VIX_indx_close'] > 20, 'regime_vix'] = 1  # Elevated\n",
    "        df.loc[df['VIX_indx_close'] > 30, 'regime_vix'] = 2  # High fear\n",
    "        df.loc[df['VIX_indx_close'] > 40, 'regime_vix'] = 3  # Panic\n",
    "    \n",
    "    # ==========================================\n",
    "    # SUMMARY\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n Added regime features\")\n",
    "    print(f\" Data shape: {df.shape}\")\n",
    "    print(f\"\\n Regime Distribution:\")\n",
    "    \n",
    "    for code, count in df['regime_combined'].value_counts().sort_index().items():\n",
    "        regime_name = regime_names.get(code, 'Unknown')\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {code}: {regime_name:25s} - {count:4d} days ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return df\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36144753-4f42-4f7a-8019-b768763dcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Select Features By Horizon ##################################\n",
    "def select_features_by_horizon(data_df, horizon=None):\n",
    "    \"\"\"\n",
    "    Select optimal features based on prediction horizon\n",
    "    Features ordered by cross-category importance\n",
    "    \"\"\"\n",
    "    \n",
    "    if horizon is None:\n",
    "        horizon = parameters['forecast_horizon']\n",
    "    \n",
    "    # ==========================================\n",
    "    # Short-term features (horizon <= 7)\n",
    "    # Ordered by: immediate impact + cross-category usefulness\n",
    "    # ==========================================\n",
    "    short_term = [\n",
    "        # Top 5 (useful for ALL horizons - momentum & regime changes)\n",
    "        'regime_transition',      # #1 - Immediate regime shifts\n",
    "        'regime_vix',            # #2 - Fear gauge (quick mover)\n",
    "        'VIX_indx_close_pct()',  # #3 - VIX returns\n",
    "        'gap',                   # #4 - Overnight gaps\n",
    "        'volume_ratio',          # #5 - Volume spikes\n",
    "        \n",
    "        # Next tier (strong short-term signals)\n",
    "        'trend_momentum',        # Trend acceleration\n",
    "        'RSI_7_norm',           # Short-term overbought/oversold\n",
    "        'MACD_diff_norm',       # MACD momentum\n",
    "        'gap_filled',           # Gap behavior\n",
    "        'BB_squeeze',           # Volatility compression\n",
    "        \n",
    "        # Technical indicators (pure short-term)\n",
    "        'RSI_14_norm',\n",
    "        'MACD_norm',\n",
    "        #'MACD_signal_norm',     # droped\n",
    "        'ROC_10',\n",
    "        'body',\n",
    "        'upper_wick',\n",
    "        'lower_wick',\n",
    "        'close_position',\n",
    "        'BB_position',\n",
    "        'BB_width',\n",
    "        'price_vs_EMA10',\n",
    "        'EMA_cross',\n",
    "        'volume_trend',\n",
    "        'vol_regime',\n",
    "        'daily_range',\n",
    "        'range_pct_change',\n",
    "        'ADX_norm',\n",
    "    ]\n",
    "    \n",
    "    # ==========================================\n",
    "    # Medium-term features (horizon <= 20)\n",
    "    # Ordered by: trend stability + cross-category usefulness\n",
    "    # ==========================================\n",
    "    medium_term = [\n",
    "        # Top 5 (useful for ALL horizons - persistent trends)\n",
    "        'regime_trend',          # #1 - Current trend state\n",
    "        'regime_combined',       # #2 - Overall market regime\n",
    "        'trend_strength_ema',    # #3 - Trend magnitude (from regime features)\n",
    "        'price_vs_SMA20',        # #4 - Price vs 20-day trend\n",
    "        'MA_cross',              # #5 - Moving average crossover\n",
    "        \n",
    "        # Next tier (medium-term dynamics)\n",
    "        'regime_volatility',     # Current vol regime\n",
    "        'regime_duration',       # Regime persistence\n",
    "        'volatility_20d',        # 20-day volatility\n",
    "        'price_vs_EMA20',\n",
    "        'ROC_20',\n",
    "        'Open_return',\n",
    "        'High_return',\n",
    "        'Low_return',\n",
    "        \n",
    "        # Market internals\n",
    "        'high_close_ratio',\n",
    "        'low_close_ratio',\n",
    "        'OBV_base_dif',\n",
    "        'OBV_futures_dif',\n",
    "        \n",
    "        # Related markets\n",
    "        'GOLD_fut_close_pct()',\n",
    "        'DXY_indx_close_pct()',\n",
    "        'S&P_fut_close_pct()',\n",
    "        'S&P_fut_vol_pct()',\n",
    "        'Volume_pct()',\n",
    "        'front_spot',\n",
    "        'gold_vix',\n",
    "        'fut_to_vix_ratio',\n",
    "        'risk_regime',\n",
    "    ]\n",
    "    \n",
    "    # ==========================================\n",
    "    # Long-term features (horizon > 20)\n",
    "    # Ordered by: macro stability + cross-category usefulness\n",
    "    # ==========================================\n",
    "    long_term = [\n",
    "        # Top 5 (useful for ALL horizons - macro context)\n",
    "        'vol_percentile',         # #1 - Long-term vol context (252-day)\n",
    "        'T10Y2Y_yield_curve',    # #2 - Yield curve (recession signal)\n",
    "        'rate_regime',           # #3 - Interest rate environment\n",
    "        'curve_regime',          # #4 - Yield curve regime\n",
    "        'term_spread',           # #5 - Credit spreads\n",
    "        \n",
    "        # Next tier (fundamental trends)\n",
    "        'trend_strength_sma',    # - Trend magnitude long (from regime features)\n",
    "        #'price_vs_SMA50',        # droped\n",
    "        'DGS10_pct()',\n",
    "        'T10Y2Y',\n",
    "        'T10Y2Y_diff',\n",
    "        \n",
    "        # Economic indicators\n",
    "        'GDP_value_pct()',\n",
    "        'CPIAUCSL_value_pct()',\n",
    "        'PAYEMS_value_pct()',\n",
    "        'UNRATE_value_diff',\n",
    "        \n",
    "        # Calendar effects\n",
    "        'is_GDP_release_day',\n",
    "        'is_CPIAUCSL_release_day',\n",
    "        'is_UNRATE_release_day',\n",
    "        'period_CPI',\n",
    "        'period_PAYMES_UNRATE',\n",
    "        'period_GDP',\n",
    "    ]\n",
    "    \n",
    "    # ==========================================\n",
    "    # Feature Selection Logic\n",
    "    # ==========================================\n",
    "    \n",
    "    # Always include target\n",
    "    selected = ['target']\n",
    "    \n",
    "    if horizon <= 7:\n",
    "        # Short-term model: All short + top medium + top long\n",
    "        selected += short_term\n",
    "        selected += medium_term[:10]  # Top 10 medium (includes regime features)\n",
    "        selected += long_term[:5]     # Top 5 long (macro context)\n",
    "        \n",
    "    elif horizon <= 20:\n",
    "        # Medium-term model: Top short + all medium + top long\n",
    "        selected += short_term[:10]   # Top 10 short (includes regime transitions)\n",
    "        selected += medium_term       # All medium features\n",
    "        selected += long_term[:12]    # Top 12 long (extended macro)\n",
    "        \n",
    "    else:  # horizon > 20\n",
    "        # Long-term model: Top short + all medium + all long\n",
    "        selected += short_term[:5]    # Top 5 short (regime signals)\n",
    "        selected += medium_term       # All medium (regime context)\n",
    "        selected += long_term         # All long-term features\n",
    "    \n",
    "    # Filter to only columns that exist in dataframe\n",
    "    selected = [col for col in selected if col in data_df.columns]\n",
    "\n",
    "    # Check for missing columns\n",
    "    missing = [col for col in selected if col not in data_df.columns \n",
    "           and col != 'target']\n",
    "    if missing:\n",
    "        print(f\"WARNING: {len(missing)} features not found:\")\n",
    "        for feat in missing[:5]:\n",
    "            print(f\"  - {feat}\")\n",
    "    \n",
    "#---------------------------------- me ----------------------------------------------------\n",
    "    # 4. Move 'target' to end\n",
    "    selected_cols = [col for col in selected if col != 'target'] + ['target']\n",
    "    \n",
    "#---------------------------------- me ----------------------------------------------------\n",
    "\n",
    "    \n",
    "    print(f\"   Selected {len(selected_cols)} features ('target' included) for {horizon}-day horizon\")\n",
    "    print(f\"   Short-term: {sum(1 for f in selected_cols if f in short_term)}\")\n",
    "    print(f\"   Medium-term: {sum(1 for f in selected_cols if f in medium_term)}\")\n",
    "    print(f\"   Long-term: {sum(1 for f in selected_cols if f in long_term)}\")\n",
    "    \n",
    "    return data_df[selected_cols]\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67a57d-8dfd-4584-bb83-dab519914a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Check if any NaN exists in the entire DataFrame ##########################\n",
    "def check_nan(df):\n",
    "    nan_mask = df.isna()\n",
    "    has_nan = nan_mask.any().any()\n",
    "    print(f\"Does the DataFrame contain any NaN values? {has_nan}\")\n",
    "    if has_nan:\n",
    "        nan_counts = nan_mask.sum()\n",
    "        print(f\"Count of NaN values per column:\\n{nan_counts[nan_counts > 0]}\")\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bcb0cf-f751-456b-aa0e-5fe2aae83041",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Get Optimal Time Step ####################################\n",
    "def get_optimal_timestep(horizon):\n",
    "    \"\"\"\n",
    "    Dynamic time step based on horizon\n",
    "    Uses 4-6x multiplier for optimal pattern recognition\n",
    "    \"\"\"\n",
    "    if horizon <= 5:\n",
    "        return max(30, horizon * 6)  # 6x for short-term\n",
    "    elif horizon <= 20:\n",
    "        return max(60, horizon * 4)  # 4x for medium-term\n",
    "    else:  # horizon > 20\n",
    "        return max(90, horizon * 4)  # 4x for long-term\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e4ab1-63d5-4b94-890e-545ba65a440d",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f04f7e-ff3d-4ccc-9f63-6eb5fd5ee076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PIPELINE EXECUTION\n",
    "# ============================================\n",
    "# 1. Load and prepare data (tatal 44 features)\n",
    "data_data = pd.read_csv('data_df_features.csv', index_col=\"Date\", parse_dates=True)\n",
    "data_data_cols = data_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba2e5c-b376-4036-8f79-f7b03fd0a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fundamentals regime (add 4 more features)\n",
    "data_df_MACRO = add_macro_regime_features_CORR(data_data)\n",
    "# Add OHLC features (13 features added)\n",
    "data_df_ohlc = add_ohlc_features_CORR(data_df_MACRO)\n",
    "# Add TA features (19 features added)\n",
    "data_df_ta = add_technical_momentum_features_CORR(data_df_ohlc)\n",
    "# Add Market Regime features (10 features added)\n",
    "data_df_mr = add_market_regime_features_CORR(data_df_ta)\n",
    "# Copy\n",
    "data_df = data_df_mr.copy()\n",
    "# Check in any NaN\n",
    "check_nan(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189035e-2fd0-4be6-9b62-e43a14edc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare returns-based features\n",
    "features, close_prices = prepare_returns_features_OPT(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31cf43e-2067-4ff6-887f-28b4d46917d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_fun(features.drop(columns=['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d65c50-7a87-42c8-a3a3-1b8ea4b7d2eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop high correlated columns\n",
    "features_total = features.drop(columns=['macro_pressure', 'is_PAYEMS_release_day', 'vol_rank', 'MACD_signal_norm', 'price_vs_SMA50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2dd014-c070-4df0-a7e1-efb11e055bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFeatures prepared:\")\n",
    "print(\"   Original data shape:\", data_df.shape)\n",
    "print(\"   Features+target data shape:\", features_total.shape)\n",
    "print(f\"   \\nColumns: {list(features_total.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a836a2-5287-4d73-8b30-d4f545422f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set parameters for Nested CV\n",
    "forecast_horizon = 45  # 20, 45\n",
    "\n",
    "parameters = {\n",
    "    'forecast_horizon': forecast_horizon,\n",
    "    'time_step': get_optimal_timestep(forecast_horizon),\n",
    "    'train_window': 1500,\n",
    "    'embargo_prop': 0.10,\n",
    "    # step_size_type: 'nested'   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf3b86-4150-4e8d-8c17-7ebf03a2f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features by horizon\n",
    "features_selected = select_features_by_horizon(features_total)    \n",
    "features_df = features_selected.copy()\n",
    "features_df_cols = features_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ccb87-4bcd-4689-9be4-b5cb6b07cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Run PWFCV\n",
    "best_hp, folds = run_pwfcv_lstm_returns_OPT(\n",
    "    features_df, \n",
    "    parameters, \n",
    "    n_samples=10     # For hp \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e54e2-5f23-42db-a276-853e89fe3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6. Split data for final training\n",
    "train_size = int(len(features_df) * 0.8)\n",
    "train_features = features_df.iloc[:train_size]\n",
    "test_features = features_df.iloc[train_size:]\n",
    "train_prices = close_prices.iloc[:train_size]\n",
    "test_prices = close_prices.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8cce6-e372-44a4-839c-5528110414ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train final model\n",
    "#-------------------------------------\n",
    "# Option 1. Uses dummy validation data\n",
    "#-------------------------------------\n",
    "'''\n",
    "final_model, target_scaler, features_scaler = retrain_final_model_returns(\n",
    "    train_features,\n",
    "    parameters,\n",
    "    best_hp,\n",
    "    use_early_stopping=True\n",
    ")\n",
    "'''\n",
    "\n",
    "#-------------------------------------\n",
    "# Option 2. Uses dedicated functions for cleanest implementation\n",
    "#-------------------------------------\n",
    "final_model, target_scaler, features_scaler = retrain_final_model_returns_CLEAN(\n",
    "    train_features,\n",
    "    parameters,\n",
    "    best_hp,\n",
    "    use_early_stopping=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83796c5c-49b0-4fd5-9a4b-cfd6a8eb2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CHECK 1: Model Summary\n",
    "----------------------\n",
    "'''\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef206e-7463-4acf-8ac8-414637dd2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Evaluate on full test set (rolling)\n",
    "actuals_ret, preds_ret, actuals_prices, preds_prices, _ = evaluate_rolling_test_returns_OPT(\n",
    "    final_model,\n",
    "    train_features,\n",
    "    test_features,\n",
    "    train_prices,\n",
    "    test_prices,\n",
    "    parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ceb3c-15cf-41f4-8ccc-1c8e4f2f2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Evaluate on first prediction\n",
    "y_true_ret, y_pred_ret, y_true_prices, y_pred_prices, _ = evaluate_on_test_returns(\n",
    "    final_model,\n",
    "    train_features,\n",
    "    test_features,\n",
    "    train_prices,\n",
    "    test_prices,\n",
    "    parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1499b-34d4-4624-bb28-fcce258193ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Analyze direction\n",
    "actual_changes, pred_changes, direction_stats = analyze_direction_accuracy_CORRECTED(\n",
    "    actuals_prices,  # shape: (n_windows, horizon)\n",
    "    preds_prices, \n",
    "    #target_scaler\n",
    "    #horizon=parameters['forecast_horizon']; horizon from  actuals_prices.shape inside the function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315a684-10cc-4050-8b23-388e57f1f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual changes range: [{actual_changes.min():.2f}, {actual_changes.max():.2f}]\")\n",
    "print(f\"Pred changes range: [{pred_changes.min():.2f}, {pred_changes.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92549648-f790-4448-a491-6943e42361ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Plot results\n",
    "#plot_direction_analysis(actuals_prices, preds_prices, target_scaler)\n",
    "plot_direction_analysis_CORR(actuals_prices, preds_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c23e9-3d62-4d14-8465-a83036aff15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09e657-4164-4da5-8ec2-e6bd30980259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bf584-4620-4e5e-a161-aea9cb079da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39efb29-d2b4-41dd-9c0d-af2a1e587161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b1e63-2cab-4a3b-a826-704d08f9a92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f63b2-d4de-47ac-b99c-261c7f015305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76029414-5af3-47e3-a52b-436f8cf2dd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finkin_env]",
   "language": "python",
   "name": "conda-env-finkin_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
